{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-02 19:57:25,270 func.utils 347 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func.utils import get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename, logger_func\n",
    "from ieee_train import eval_train, eval_check_feature\n",
    "from kaggle_utils import reduce_mem_usage, move_feature\n",
    "logger = logger_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807\n"
     ]
    }
   ],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMN_GROUP = 'DT-M'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, COLUMN_GROUP, 'is_train', 'date']\n",
    "\n",
    "def filter_feature(path):\n",
    "    if path.count(''):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# paths_train = glob('../submit/re_sub/50*_train.gz')\n",
    "# paths_test  = glob('../submit/re_sub/50*_test.gz')\n",
    "# paths_train += glob('../submit/re_sub/Tran*_train.gz')\n",
    "# paths_test  += glob('../submit/re_sub/Tran*_test.gz')\n",
    "# paths_train += glob('../submit/re_sub/is*_train.gz')\n",
    "# paths_test  += glob('../submit/re_sub/is*_test.gz')\n",
    "\n",
    "paths_train = glob('../submit/re_sub/*_train.gz')\n",
    "paths_test  = glob('../submit/re_sub/*_test.gz')\n",
    "# paths_train += glob('../submit/add_feature/*_train.gz')\n",
    "# paths_test  += glob('../submit/add_feature/*_test.gz')\n",
    "\n",
    "print(len(paths_train))\n",
    "# sys.exit()\n",
    "# paths_train += glob('../feature/valid_trush/528*uid2*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_trush/528*uid2*_test.gz')\n",
    "\n",
    "# for path in paths_train:\n",
    "#     if path.count('C14_ratio'):\n",
    "#         paths_train.remove(path)\n",
    "        \n",
    "# for path in paths_test:\n",
    "#     if path.count('C14_ratio'):\n",
    "#         paths_test.remove(path)\n",
    "    \n",
    "# paths_train = glob('../feature/raw_use/*_train.gz')\n",
    "# paths_test = glob('../feature/raw_use/*_test.gz')\n",
    "# paths_train = [path for path in paths_train if filter_feature(path) ]\n",
    "# paths_test = [path for path in paths_test if filter_feature(path) ]\n",
    "\n",
    "# paths_train_feature = sorted(glob('../feature/org_use/*_train.gz'))\n",
    "# paths_test_feature  = sorted(glob('../feature/org_use/*_test.gz'))\n",
    "\n",
    "# paths_train_feature += sorted(glob('../feature/valid/*_train.gz'))\n",
    "# paths_test_feature  += sorted(glob('../feature/valid/*_test.gz'))\n",
    "\n",
    "# paths_train_feature += sorted(glob('../feature/kernel/*_train.gz'))\n",
    "# paths_test_feature  += sorted(glob('../feature/kernel/*_test.gz'))\n",
    "\n",
    "# paths_train_feature = sorted(glob('../feature/valid_use/*_train.gz'))\n",
    "# paths_test_feature  = sorted(glob('../feature/valid_use/*_test.gz'))\n",
    "paths_train_feature = []\n",
    "paths_test_feature  = []\n",
    "\n",
    "# df_train = reduce_mem_usage( parallel_load_data(paths_train) )\n",
    "# df_test  = reduce_mem_usage( parallel_load_data(paths_test) )\n",
    "df_train = parallel_load_data(paths_train)\n",
    "df_test  = parallel_load_data(paths_test)\n",
    "Y = df_train[COLUMN_TARGET]\n",
    "df_train.drop(COLUMN_TARGET, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same_user_path = '../output/same_user_pattern/20190901_user_ids_share.csv'\n",
    "# same_user_path = '../output/same_user_pattern/20190901_user_ids_share.csv'\n",
    "# bear = pd.read_csv(same_user_path)\n",
    "# bear = bear[[COLUMN_ID, 'predicted_user_id']]\n",
    "# max_id = bear['predicted_user_id'].max()\n",
    "# bear.loc[bear[bear['predicted_user_id'].isnull()].index, 'predicted_user_id'] = np.arange(\n",
    "#     bear['predicted_user_id'].isnull().sum() ) + 1 + max_id\n",
    "# bear['predicted_user_id'] =  bear['predicted_user_id'].astype('int')\n",
    "# bear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Diff Features: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-02 19:59:20,235 func.utils 112 [INFO]    [<module>] * EXP: dataset new_set (590540, 1807) lr 0.01  \n"
     ]
    }
   ],
   "source": [
    "is_submit = [True, False][0]\n",
    "n_splits = 5\n",
    "set_type = 'new_set'\n",
    "\n",
    "valid_paths_train = paths_train_feature[:]\n",
    "valid_paths_test  = paths_test_feature[:]\n",
    "\n",
    "#========================================================================\n",
    "# pathの存在チェック。なぜかたびたびFileNotFoundErrorが起きるので,,,\n",
    "#========================================================================\n",
    "remove_paths = []\n",
    "for trn_path, tes_path in zip(valid_paths_train, valid_paths_test):\n",
    "    if os.path.exists(trn_path) and os.path.exists(tes_path):\n",
    "        pass\n",
    "    else:\n",
    "        remove_paths.append(trn_path)\n",
    "        remove_paths.append(tes_path)\n",
    "for path in remove_paths:\n",
    "    if path.count('train'):\n",
    "        valid_paths_train.remove(path)\n",
    "        print(f'remove {path}')\n",
    "    elif path.count('test'):\n",
    "        valid_paths_test.remove(path)\n",
    "        print(f'remove {path}')\n",
    "\n",
    "if len(valid_paths_train):\n",
    "    df_feat_train = parallel_load_data(valid_paths_train)\n",
    "    df_feat_test  = parallel_load_data(valid_paths_test)\n",
    "    \n",
    "    col_drops = eval_check_feature(df_feat_train, df_feat_test)\n",
    "    \n",
    "    tmp_train = df_train.join(df_feat_train)\n",
    "    tmp_test = df_test.join(df_feat_test)\n",
    "else:\n",
    "    tmp_train = df_train\n",
    "    tmp_test = df_test\n",
    "\n",
    "#========================================================================\n",
    "# Train Test で片方に存在しないFeatureを除外\n",
    "#========================================================================\n",
    "diff_cols = list(set(tmp_train.columns) - set(tmp_test.columns))\n",
    "\n",
    "for col in list(set(diff_cols)):\n",
    "    from_dir = 'valid'\n",
    "    to_dir = 'valid_trush'\n",
    "    move_feature([col], from_dir, to_dir)\n",
    "tmp_train.drop(diff_cols, axis=1, inplace=True)\n",
    "print(f\"  * Diff Features: {len(diff_cols)}\")\n",
    "\n",
    "# same_user_path = '../output/same_user_pattern/0902__same_user_id__card_addr_pemail_M.csv'\n",
    "# tmp_train = tmp_train.merge(bear[[COLUMN_ID, 'predicted_user_id']], how='inner', on=COLUMN_ID)\n",
    "# COLUMN_GROUP = 'predicted_user_id'\n",
    "# COLUMNS_IGNORE.append('predicted_user_id')\n",
    "\n",
    "### DT-M\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "tmp_train[COLUMN_GROUP] = group\n",
    "\n",
    "# tmp_train[COLUMN_GROUP] = tmp_train['528__ugr_uid3_Regist_date_agg_V95_137_mean_mean'].fillna(0)\n",
    "\n",
    "#========================================================================\n",
    "# Features elimination \n",
    "#==============================================================\n",
    "# from scipy.stats import ks_2samp\n",
    "# features_check = []\n",
    "# columns_to_check = set(list(tmp_train)).difference(COLUMNS_IGNORE)\n",
    "# for i in columns_to_check:\n",
    "#     features_check.append(ks_2samp(tmp_test[i], tmp_train[i])[1])\n",
    "\n",
    "# features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "# features_discard = list(features_check[features_check==0].index)\n",
    "# print(features_discard)\n",
    "# tmp_train.drop(features_discard, axis=1, inplace=True)\n",
    "# tmp_test.drop(features_discard, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "model_type = \"lgb\"\n",
    "params = {\n",
    "#     'n_jobs': 60,\n",
    "    'n_jobs': 96,\n",
    "#     'n_jobs': 84,\n",
    "#     'n_jobs': 48,\n",
    "#     'n_jobs': 36,\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 2**8-1,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.9,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree' : 0.10,\n",
    "    'lambda_l1' : 0.1,\n",
    "    'lambda_l2' : 1.0,\n",
    "    'learning_rate' : 0.1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"seed\": 1208,\n",
    "    \"bagging_seed\": 1208,\n",
    "    \"feature_fraction_seed\": 1208,\n",
    "    \"drop_seed\": 1208,\n",
    "    'n_splits': n_splits,\n",
    "    'metric': 'auc',\n",
    "    'model_type': model_type,\n",
    "    'fold': ['stratified', 'group'][1],\n",
    "}\n",
    "if is_submit:\n",
    "    params['learning_rate'] = 0.01\n",
    "#     params['learning_rate'] = 0.05\n",
    "#     params['learning_rate'] = 0.1\n",
    "    params[\"early_stopping_rounds\"] = 100\n",
    "\n",
    "\n",
    "use_cols = [col for col in df_train.columns if col not in COLUMNS_IGNORE]\n",
    "logger.info(f\"* EXP: dataset {set_type} {tmp_train.shape} lr {params['learning_rate']} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.910771\n",
      "[400]\tvalid_0's auc: 0.924121\n",
      "[600]\tvalid_0's auc: 0.930409\n",
      "[800]\tvalid_0's auc: 0.93349\n",
      "[1000]\tvalid_0's auc: 0.934975\n",
      "[1200]\tvalid_0's auc: 0.936173\n",
      "[1400]\tvalid_0's auc: 0.936863\n",
      "[1600]\tvalid_0's auc: 0.937323\n",
      "[1800]\tvalid_0's auc: 0.937532\n",
      "[2000]\tvalid_0's auc: 0.937821\n",
      "[2200]\tvalid_0's auc: 0.937875\n",
      "Early stopping, best iteration is:\n",
      "[2129]\tvalid_0's auc: 0.938073\n",
      "[  * Fold0 Validation-DT-M 2017-12: 134339] done in 651 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-02 20:10:25,383 func.utils 168 [INFO]    [ieee_cv]   * Fold0 2017-12: 0.9380729263135189 | Bear's...PB:0.9866347280926046 PV:0.9830166404120283 All:0.9842680581907638 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "feim, _ = eval_train(\n",
    "    logger,\n",
    "    tmp_train,\n",
    "    Y,\n",
    "    tmp_test,\n",
    "    COLUMN_GROUP,\n",
    "    model_type,\n",
    "    params,\n",
    "    is_adv=[True, False][1],\n",
    "    is_viz=[True, False][0],\n",
    ")\n",
    "feim = list_result_feim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>is_multi_section_user</th>\n",
       "      <th>fixed_D1</th>\n",
       "      <th>section_id</th>\n",
       "      <th>section_row_id</th>\n",
       "      <th>predicted_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3189865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3194665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3241275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3251171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3322570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  is_multi_section_user  fixed_D1  section_id  section_row_id  \\\n",
       "0        3189865                    NaN       NaN         NaN             NaN   \n",
       "1        3194665                    NaN       NaN         NaN             NaN   \n",
       "2        3241275                    NaN       NaN         NaN             NaN   \n",
       "3        3251171                    NaN       NaN         NaN             NaN   \n",
       "4        3322570                    NaN       NaN         NaN             NaN   \n",
       "\n",
       "   predicted_user_id  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bear = pd.read_csv('../output/same_user_pattern/20190901_user_ids_share.csv')\n",
    "bear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>predicted_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>354062</th>\n",
       "      <td>3421965</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354063</th>\n",
       "      <td>3421966</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354064</th>\n",
       "      <td>3421968</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354065</th>\n",
       "      <td>3427288</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354066</th>\n",
       "      <td>3432633</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354074</th>\n",
       "      <td>3432653</td>\n",
       "      <td>97703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354075</th>\n",
       "      <td>3432877</td>\n",
       "      <td>97703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354067</th>\n",
       "      <td>3433038</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354068</th>\n",
       "      <td>3433041</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354069</th>\n",
       "      <td>3434804</td>\n",
       "      <td>97701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354076</th>\n",
       "      <td>3437210</td>\n",
       "      <td>97703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354077</th>\n",
       "      <td>3437213</td>\n",
       "      <td>97703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354055</th>\n",
       "      <td>3438228</td>\n",
       "      <td>97700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354056</th>\n",
       "      <td>3438381</td>\n",
       "      <td>97700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354057</th>\n",
       "      <td>3438382</td>\n",
       "      <td>97700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354058</th>\n",
       "      <td>3439573</td>\n",
       "      <td>97700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354059</th>\n",
       "      <td>3447033</td>\n",
       "      <td>97700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354060</th>\n",
       "      <td>3469633</td>\n",
       "      <td>97700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID  predicted_user_id\n",
       "354062        3421965            97701.0\n",
       "354063        3421966            97701.0\n",
       "354064        3421968            97701.0\n",
       "354065        3427288            97701.0\n",
       "354066        3432633            97701.0\n",
       "354074        3432653            97703.0\n",
       "354075        3432877            97703.0\n",
       "354067        3433038            97701.0\n",
       "354068        3433041            97701.0\n",
       "354069        3434804            97701.0\n",
       "354076        3437210            97703.0\n",
       "354077        3437213            97703.0\n",
       "354055        3438228            97700.0\n",
       "354056        3438381            97700.0\n",
       "354057        3438382            97700.0\n",
       "354058        3439573            97700.0\n",
       "354059        3447033            97700.0\n",
       "354060        3469633            97700.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probing[probing['TransactionID']==3421965]\n",
    "bear[bear['TransactionID'].isin(ids)].sort_values(by=COLUMN_ID)[[COLUMN_ID, 'predicted_user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>data_type</th>\n",
       "      <th>Probing_isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TransactionID, transaction_time, user_id, data_type, Probing_isFraud]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probing = pd.read_csv('../input/20190929_probing.csv')\n",
    "no_probing = pd.read_csv('../input/20190929_no_probing.csv')\n",
    "ids = [\n",
    "3421965,\n",
    "3421966,\n",
    "3421968,\n",
    "3427288,\n",
    "3432633,\n",
    "3432653,\n",
    "3432877,\n",
    "3433038,\n",
    "3433041,\n",
    "3434804,\n",
    "3437210,\n",
    "3437213,\n",
    "3438228,\n",
    "3438381,\n",
    "3438382,\n",
    "3439573,\n",
    "3447033,\n",
    "3469633,\n",
    "]\n",
    "probing[probing['TransactionID'].astype('int').isin(ids)]\n",
    "no_probing[no_probing['TransactionID'].astype('int').isin(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feim = list_result_feim[0]\n",
    "# feim = read_pkl_gzip('../output/feature_importances/20190926_1350__CV0-9604930198337085__feature1942.gz')\n",
    "# feim = read_pkl_gzip('../output/feature_importances/20190925_1450__CV0-9581588018233685__feature2114.gz')\n",
    "# key1 = '532_'\n",
    "# key2 = '_D'\n",
    "\n",
    "# idx = [col for col in feim.index if\n",
    "#        (\n",
    "#        col.count(key1)\n",
    "# #        and\n",
    "#        or\n",
    "#        col.count(key2)\n",
    "#        )\n",
    "#        and not col.count('ratio')\n",
    "#        and not col.count('diff')\n",
    "#       ]\n",
    "# feim = feim.loc[idx]\n",
    "# feim.sort_values(by='imp_avg', inplace=True, ascending=False)\n",
    "# print(feim.shape)\n",
    "# feim.tail(100)\n",
    "feim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6ef9b7027894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# feim = read_pkl_gzip('../output/feature_importances/20190909_2324__CV0-9467296784440689__feature552.gz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# for feature_name in feim.tail(100).index:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for feature_name in feim[feim['imp_avg']<6000].index:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feim' is not defined"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "# feim = read_pkl_gzip('../output/feature_importances/20190909_2324__CV0-9467296784440689__feature552.gz')\n",
    "for feature_name in feim.index:\n",
    "# for feature_name in feim.tail(100).index:\n",
    "# for feature_name in feim[feim['imp_avg']<6000].index:\n",
    "    \n",
    "    if not feature_name.count('mean'):\n",
    "        continue\n",
    "\n",
    "    if feature_name.count('raw'):\n",
    "        from_dir = 'raw_use'\n",
    "        to_dir = 'raw_trush'\n",
    "#         from_dir = 'raw_trush'\n",
    "#         to_dir = 'raw_use'\n",
    "    else:\n",
    "        from_dir = 'org_use'\n",
    "        to_dir = 'org_trush'\n",
    "#         from_dir = 'org_trush'\n",
    "#         to_dir = 'org_use'\n",
    "#     from_dir = 'product_feature'\n",
    "#     from_dir = 'kernel'\n",
    "#     from_dir = 'check_trush'\n",
    "#     from_dir = 'raw_trush'\n",
    "#     from_dir = 'raw_use'\n",
    "#     from_dir = 'org_trush'\n",
    "#     from_dir = 'useless'\n",
    "    from_dir = '../submit/re_sub'\n",
    "#     from_dir = '../submit/add_feature/'\n",
    "#     from_dir = 'org_use'\n",
    "#     from_dir = 'valid_use'\n",
    "#     from_dir = 'valid_trush'\n",
    "#     to_dir = '../submit/escape'\n",
    "#     to_dir = '../submit/add_feature'\n",
    "#     to_dir = '../submit/add_feature'\n",
    "#     to_dir = 'valid_trush'\n",
    "#     to_dir = 'valid_use'\n",
    "#     to_dir = 'raw_trush'\n",
    "#     to_dir = 'check_trush'\n",
    "#     to_dir = 'org_use'\n",
    "#     to_dir = 'useless'\n",
    "#     to_dir = 'valid'\n",
    "#     to_dir = 'kernel'\n",
    "    try:\n",
    "#         print(feature_name)\n",
    "        move_feature([feature_name], from_dir, to_dir)\n",
    "        cnt+=1\n",
    "    except FileNotFoundError:\n",
    "        print(feature_name)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Diff Features: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 06:04:52,252 func.utils 111 [INFO]    [<module>] * EXP: dataset new_set (590540, 1682) lr 0.01  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.905843\n",
      "[400]\tvalid_0's auc: 0.919909\n",
      "[600]\tvalid_0's auc: 0.926924\n",
      "[800]\tvalid_0's auc: 0.930362\n"
     ]
    }
   ],
   "source": [
    "is_submit = [True, False][0]\n",
    "has_dec = [True, False][0]\n",
    "n_splits = 6\n",
    "set_type = 'new_set'\n",
    "\n",
    "valid_paths_train = paths_train_feature[:]\n",
    "valid_paths_test  = paths_test_feature[:]\n",
    "\n",
    "#========================================================================\n",
    "# pathの存在チェック。なぜかたびたびFileNotFoundErrorが起きるので,,,\n",
    "#========================================================================\n",
    "remove_paths = []\n",
    "for trn_path, tes_path in zip(valid_paths_train, valid_paths_test):\n",
    "    if os.path.exists(trn_path) and os.path.exists(tes_path):\n",
    "        pass\n",
    "    else:\n",
    "        remove_paths.append(trn_path)\n",
    "        remove_paths.append(tes_path)\n",
    "for path in remove_paths:\n",
    "    if path.count('train'):\n",
    "        valid_paths_train.remove(path)\n",
    "        print(f'remove {path}')\n",
    "    elif path.count('test'):\n",
    "        valid_paths_test.remove(path)\n",
    "        print(f'remove {path}')\n",
    "\n",
    "if len(valid_paths_train):\n",
    "    df_feat_train = parallel_load_data(valid_paths_train)\n",
    "    df_feat_test  = parallel_load_data(valid_paths_test)\n",
    "    \n",
    "    col_drops = eval_check_feature(df_feat_train, df_feat_test)\n",
    "    \n",
    "    tmp_train = df_train.join(df_feat_train)\n",
    "    tmp_test = df_test.join(df_feat_test)\n",
    "else:\n",
    "    tmp_train = df_train\n",
    "    tmp_test = df_test\n",
    "\n",
    "#========================================================================\n",
    "# Train Test で片方に存在しないFeatureを除外\n",
    "#========================================================================\n",
    "diff_cols = list(set(tmp_train.columns) - set(tmp_test.columns))\n",
    "\n",
    "for col in list(set(diff_cols)):\n",
    "    from_dir = 'valid'\n",
    "    to_dir = 'valid_trush'\n",
    "    move_feature([col], from_dir, to_dir)\n",
    "tmp_train.drop(diff_cols, axis=1, inplace=True)\n",
    "print(f\"  * Diff Features: {len(diff_cols)}\")\n",
    "\n",
    "# same_user_path = '../output/same_user_pattern/20190901_user_ids_share.csv'\n",
    "# same_user_path = '../output/same_user_pattern/0902__same_user_id__card_addr_pemail_M.csv'\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "tmp_train[COLUMN_GROUP] = group\n",
    "\n",
    "# 2017-12抜いてみる\n",
    "# if not has_dec:\n",
    "#     tmp_train = tmp_train[tmp_train[COLUMN_GROUP]!='2017-12']\n",
    "#     Y = Y.loc[tmp_train.index]\n",
    "#     n_splits = 5\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Features elimination \n",
    "#==============================================================\n",
    "# from scipy.stats import ks_2samp\n",
    "# features_check = []\n",
    "# columns_to_check = set(list(tmp_train)).difference(COLUMNS_IGNORE)\n",
    "# for i in columns_to_check:\n",
    "#     features_check.append(ks_2samp(tmp_test[i], tmp_train[i])[1])\n",
    "\n",
    "# features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "# features_discard = list(features_check[features_check==0].index)\n",
    "# print(features_discard)\n",
    "# tmp_train.drop(features_discard, axis=1, inplace=True)\n",
    "# tmp_test.drop(features_discard, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "model_type = \"lgb\"\n",
    "params = {\n",
    "#     'n_jobs': 60,\n",
    "    'n_jobs': 96,\n",
    "#     'n_jobs': 84,\n",
    "#     'n_jobs': 48,\n",
    "#     'n_jobs': 36,\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 2**8-1,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree' : 0.10,\n",
    "    'lambda_l1' : 0.1,\n",
    "    'lambda_l2' : 1.0,\n",
    "    'learning_rate' : 0.1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"seed\": 1208,\n",
    "    \"bagging_seed\": 1208,\n",
    "    \"feature_fraction_seed\": 1208,\n",
    "    \"drop_seed\": 1208,\n",
    "    'n_splits': n_splits,\n",
    "    'metric': 'auc',\n",
    "    'model_type': model_type,\n",
    "    'fold': ['stratified', 'group'][1],\n",
    "}\n",
    "if is_submit:\n",
    "    params['learning_rate'] = 0.01\n",
    "#     params['learning_rate'] = 0.05\n",
    "    params[\"early_stopping_rounds\"] = 100\n",
    "\n",
    "logger.info(f\"* EXP: dataset {set_type} {tmp_train.shape} lr {params['learning_rate']} \")\n",
    "\n",
    "list_result_feim = eval_train(\n",
    "    logger,\n",
    "    tmp_train,\n",
    "    Y,\n",
    "    tmp_test,\n",
    "    COLUMN_GROUP,\n",
    "    model_type,\n",
    "    params,\n",
    "    is_adv=[True, False][1],\n",
    "    is_viz=[True, False][0],\n",
    ")\n",
    "feim = list_result_feim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
