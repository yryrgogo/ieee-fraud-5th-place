{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 12:38:02,853 func.utils 347 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func.utils import get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename, logger_func\n",
    "from ieee_train import eval_train, eval_check_feature\n",
    "from kaggle_utils import reduce_mem_usage, move_feature\n",
    "logger = logger_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808\n"
     ]
    }
   ],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMN_GROUP = 'DT-M'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, COLUMN_GROUP, 'is_train', 'date']\n",
    "\n",
    "def filter_feature(path):\n",
    "    if path.count(''):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# paths_train = glob('../submit/lb9516/*_train.gz')\n",
    "# paths_test  = glob('../submit/lb9516/*_test.gz')\n",
    "# paths_train = glob('../feature/raw_use/is*_train.gz')\n",
    "# paths_test  = glob('../feature/raw_use/is*_test.gz')\n",
    "# paths_train += glob('../feature/raw_use/TransactionID_train.gz')\n",
    "# paths_test  += glob('../feature/raw_use/TransactionID_test.gz')\n",
    "# paths_train = glob('../submit/sub_valid/*_train.gz')\n",
    "# paths_test  = glob('../submit/sub_valid/*_test.gz')\n",
    "paths_train = glob('../submit/re_sub/*_train.gz')\n",
    "paths_test  = glob('../submit/re_sub/*_test.gz')\n",
    "# paths_train += glob('../submit/add_feature/*_train.gz')\n",
    "# paths_test  += glob('../submit/add_feature/*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/531*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/531*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/532*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/532*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/533*uid2*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/533*uid2*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/533*uid3*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/533*uid3*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/534*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/534*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/535*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/535*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/536*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/536*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/703*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/703*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/704*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/704*_test.gz')\n",
    "\n",
    "print(len(paths_train))\n",
    "# sys.exit()\n",
    "# paths_train += glob('../feature/valid_trush/528*uid2*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_trush/528*uid2*_test.gz')\n",
    "\n",
    "# for path in paths_train:\n",
    "#     if path.count('C14_ratio'):\n",
    "#         paths_train.remove(path)\n",
    "        \n",
    "# for path in paths_test:\n",
    "#     if path.count('C14_ratio'):\n",
    "#         paths_test.remove(path)\n",
    "    \n",
    "# paths_train = glob('../feature/raw_use/*_train.gz')\n",
    "# paths_test = glob('../feature/raw_use/*_test.gz')\n",
    "# paths_train = [path for path in paths_train if filter_feature(path) ]\n",
    "# paths_test = [path for path in paths_test if filter_feature(path) ]\n",
    "\n",
    "# paths_train_feature = sorted(glob('../feature/org_use/*_train.gz'))\n",
    "# paths_test_feature  = sorted(glob('../feature/org_use/*_test.gz'))\n",
    "\n",
    "# paths_train_feature += sorted(glob('../feature/valid/*_train.gz'))\n",
    "# paths_test_feature  += sorted(glob('../feature/valid/*_test.gz'))\n",
    "\n",
    "# paths_train_feature += sorted(glob('../feature/kernel/*_train.gz'))\n",
    "# paths_test_feature  += sorted(glob('../feature/kernel/*_test.gz'))\n",
    "\n",
    "# paths_train_feature = sorted(glob('../feature/valid_use/*_train.gz'))\n",
    "# paths_test_feature  = sorted(glob('../feature/valid_use/*_test.gz'))\n",
    "paths_train_feature = []\n",
    "paths_test_feature  = []\n",
    "\n",
    "# df_train = reduce_mem_usage( parallel_load_data(paths_train) )\n",
    "# df_test  = reduce_mem_usage( parallel_load_data(paths_test) )\n",
    "df_train = parallel_load_data(paths_train)\n",
    "df_test  = parallel_load_data(paths_test)\n",
    "Y = df_train[COLUMN_TARGET]\n",
    "df_train.drop(COLUMN_TARGET, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_user_path = '../output/same_user_pattern/20190901_user_ids_share.csv'\n",
    "same_user_path = '../output/same_user_pattern/20190901_user_ids_share.csv'\n",
    "bear = pd.read_csv(same_user_path)\n",
    "bear = bear[[COLUMN_ID, 'predicted_user_id']]\n",
    "max_id = bear['predicted_user_id'].max()\n",
    "bear.loc[bear[bear['predicted_user_id'].isnull()].index, 'predicted_user_id'] = np.arange(\n",
    "    bear['predicted_user_id'].isnull().sum() ) + 1 + max_id\n",
    "bear['predicted_user_id'] =  bear['predicted_user_id'].astype('int')\n",
    "bear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 12:06:18,950 func.utils 110 [INFO]    [<module>] * EXP: dataset new_set (590540, 1808) lr 0.01  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.911292\n",
      "[400]\tvalid_0's auc: 0.923907\n",
      "[600]\tvalid_0's auc: 0.930003\n",
      "[800]\tvalid_0's auc: 0.933052\n",
      "[1000]\tvalid_0's auc: 0.934424\n",
      "[1200]\tvalid_0's auc: 0.935496\n",
      "[1400]\tvalid_0's auc: 0.9362\n",
      "Early stopping, best iteration is:\n",
      "[1454]\tvalid_0's auc: 0.936428\n",
      "[  * Fold0 Validation-DT-M 2017-12: 134339] done in 494 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 12:14:45,725 func.utils 142 [INFO]    [ieee_cv]   * Fold0 2017-12: 0.936427858108676 | Bear's...PB:0.9821016515979777 PV:0.9793187710724173 All:0.9802716516870587 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.939665\n",
      "[400]\tvalid_0's auc: 0.950241\n",
      "[600]\tvalid_0's auc: 0.955327\n",
      "[800]\tvalid_0's auc: 0.957415\n",
      "[1000]\tvalid_0's auc: 0.958305\n",
      "[1200]\tvalid_0's auc: 0.958715\n",
      "Early stopping, best iteration is:\n",
      "[1206]\tvalid_0's auc: 0.958729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 12:20:23,145 func.utils 142 [INFO]    [ieee_cv]   * Fold1 2018-3: 0.9587287359142035 | Bear's...PB:0.9733751527552958 PV:0.9766506215157055 All:0.9758840718511469 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  * Fold1 Validation-DT-M 2018-3: 101968] done in 326 s\n",
      "====================\n",
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.938764\n",
      "[400]\tvalid_0's auc: 0.952806\n",
      "[600]\tvalid_0's auc: 0.959102\n",
      "[800]\tvalid_0's auc: 0.961887\n",
      "[1000]\tvalid_0's auc: 0.963742\n",
      "[1200]\tvalid_0's auc: 0.964548\n",
      "[1400]\tvalid_0's auc: 0.96505\n",
      "[1600]\tvalid_0's auc: 0.965308\n",
      "[1800]\tvalid_0's auc: 0.965458\n",
      "[2000]\tvalid_0's auc: 0.965485\n",
      "[2200]\tvalid_0's auc: 0.965669\n",
      "Early stopping, best iteration is:\n",
      "[2110]\tvalid_0's auc: 0.965678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 12:30:22,871 func.utils 142 [INFO]    [ieee_cv]   * Fold2 2018-1: 0.9656778580281786 | Bear's...PB:0.9863639548568761 PV:0.9820448094755271 All:0.9836346021903024 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  * Fold2 Validation-DT-M 2018-1: 92510] done in 590 s\n",
      "====================\n",
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.93412\n",
      "[400]\tvalid_0's auc: 0.944926\n",
      "[600]\tvalid_0's auc: 0.949732\n",
      "[800]\tvalid_0's auc: 0.951648\n",
      "[1000]\tvalid_0's auc: 0.952949\n",
      "[1200]\tvalid_0's auc: 0.953612\n",
      "[1400]\tvalid_0's auc: 0.954084\n",
      "[1600]\tvalid_0's auc: 0.954491\n",
      "[1800]\tvalid_0's auc: 0.954794\n",
      "Early stopping, best iteration is:\n",
      "[1866]\tvalid_0's auc: 0.954852\n"
     ]
    }
   ],
   "source": [
    "is_submit = [True, False][0]\n",
    "n_splits = 6\n",
    "set_type = 'new_set'\n",
    "\n",
    "valid_paths_train = paths_train_feature[:]\n",
    "valid_paths_test  = paths_test_feature[:]\n",
    "\n",
    "#========================================================================\n",
    "# pathの存在チェック。なぜかたびたびFileNotFoundErrorが起きるので,,,\n",
    "#========================================================================\n",
    "remove_paths = []\n",
    "for trn_path, tes_path in zip(valid_paths_train, valid_paths_test):\n",
    "    if os.path.exists(trn_path) and os.path.exists(tes_path):\n",
    "        pass\n",
    "    else:\n",
    "        remove_paths.append(trn_path)\n",
    "        remove_paths.append(tes_path)\n",
    "for path in remove_paths:\n",
    "    if path.count('train'):\n",
    "        valid_paths_train.remove(path)\n",
    "        print(f'remove {path}')\n",
    "    elif path.count('test'):\n",
    "        valid_paths_test.remove(path)\n",
    "        print(f'remove {path}')\n",
    "\n",
    "if len(valid_paths_train):\n",
    "    df_feat_train = parallel_load_data(valid_paths_train)\n",
    "    df_feat_test  = parallel_load_data(valid_paths_test)\n",
    "    \n",
    "    col_drops = eval_check_feature(df_feat_train, df_feat_test)\n",
    "    \n",
    "    tmp_train = df_train.join(df_feat_train)\n",
    "    tmp_test = df_test.join(df_feat_test)\n",
    "else:\n",
    "    tmp_train = df_train\n",
    "    tmp_test = df_test\n",
    "\n",
    "#========================================================================\n",
    "# Train Test で片方に存在しないFeatureを除外\n",
    "#========================================================================\n",
    "diff_cols = list(set(tmp_train.columns) - set(tmp_test.columns))\n",
    "\n",
    "for col in list(set(diff_cols)):\n",
    "    from_dir = 'valid'\n",
    "    to_dir = 'valid_trush'\n",
    "    move_feature([col], from_dir, to_dir)\n",
    "tmp_train.drop(diff_cols, axis=1, inplace=True)\n",
    "print(f\"  * Diff Features: {len(diff_cols)}\")\n",
    "\n",
    "# same_user_path = '../output/same_user_pattern/0902__same_user_id__card_addr_pemail_M.csv'\n",
    "# tmp_train = tmp_train.merge(bear[[COLUMN_ID, 'predicted_user_id']], how='inner', on=COLUMN_ID)\n",
    "# COLUMN_GROUP = 'predicted_user_id'\n",
    "# COLUMNS_IGNORE.append('predicted_user_id')\n",
    "\n",
    "### DT-M\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "tmp_train[COLUMN_GROUP] = group\n",
    "\n",
    "# tmp_train[COLUMN_GROUP] = tmp_train['528__ugr_uid3_Regist_date_agg_V95_137_mean_mean'].fillna(0)\n",
    "\n",
    "#========================================================================\n",
    "# Features elimination \n",
    "#==============================================================\n",
    "# from scipy.stats import ks_2samp\n",
    "# features_check = []\n",
    "# columns_to_check = set(list(tmp_train)).difference(COLUMNS_IGNORE)\n",
    "# for i in columns_to_check:\n",
    "#     features_check.append(ks_2samp(tmp_test[i], tmp_train[i])[1])\n",
    "\n",
    "# features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "# features_discard = list(features_check[features_check==0].index)\n",
    "# print(features_discard)\n",
    "# tmp_train.drop(features_discard, axis=1, inplace=True)\n",
    "# tmp_test.drop(features_discard, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "model_type = \"lgb\"\n",
    "params = {\n",
    "#     'n_jobs': 60,\n",
    "    'n_jobs': 96,\n",
    "#     'n_jobs': 84,\n",
    "#     'n_jobs': 48,\n",
    "#     'n_jobs': 36,\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 2**8-1,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree' : 0.10,\n",
    "    'lambda_l1' : 0.1,\n",
    "    'lambda_l2' : 1.0,\n",
    "    'learning_rate' : 0.1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"seed\": 1208,\n",
    "    \"bagging_seed\": 1208,\n",
    "    \"feature_fraction_seed\": 1208,\n",
    "    \"drop_seed\": 1208,\n",
    "    'n_splits': n_splits,\n",
    "    'metric': 'auc',\n",
    "    'model_type': model_type,\n",
    "    'fold': ['stratified', 'group'][1],\n",
    "}\n",
    "if is_submit:\n",
    "    params['learning_rate'] = 0.01\n",
    "#     params['learning_rate'] = 0.05\n",
    "#     params['learning_rate'] = 0.1\n",
    "    params[\"early_stopping_rounds\"] = 100\n",
    "\n",
    "logger.info(f\"* EXP: dataset {set_type} {tmp_train.shape} lr {params['learning_rate']} \")\n",
    "\n",
    "feim, _ = eval_train(\n",
    "    logger,\n",
    "    tmp_train,\n",
    "    Y,\n",
    "    tmp_test,\n",
    "    COLUMN_GROUP,\n",
    "    model_type,\n",
    "    params,\n",
    "    is_adv=[True, False][1],\n",
    "    is_viz=[True, False][0],\n",
    ")\n",
    "feim = list_result_feim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feim = list_result_feim[0]\n",
    "# feim = read_pkl_gzip('../output/feature_importances/20190926_1350__CV0-9604930198337085__feature1942.gz')\n",
    "# feim = read_pkl_gzip('../output/feature_importances/20190925_1450__CV0-9581588018233685__feature2114.gz')\n",
    "# key1 = '532_'\n",
    "# key2 = '_D'\n",
    "\n",
    "# idx = [col for col in feim.index if\n",
    "#        (\n",
    "#        col.count(key1)\n",
    "# #        and\n",
    "#        or\n",
    "#        col.count(key2)\n",
    "#        )\n",
    "#        and not col.count('ratio')\n",
    "#        and not col.count('diff')\n",
    "#       ]\n",
    "# feim = feim.loc[idx]\n",
    "# feim.sort_values(by='imp_avg', inplace=True, ascending=False)\n",
    "# print(feim.shape)\n",
    "# feim.tail(100)\n",
    "feim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6ef9b7027894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# feim = read_pkl_gzip('../output/feature_importances/20190909_2324__CV0-9467296784440689__feature552.gz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# for feature_name in feim.tail(100).index:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for feature_name in feim[feim['imp_avg']<6000].index:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feim' is not defined"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "# feim = read_pkl_gzip('../output/feature_importances/20190909_2324__CV0-9467296784440689__feature552.gz')\n",
    "for feature_name in feim.index:\n",
    "# for feature_name in feim.tail(100).index:\n",
    "# for feature_name in feim[feim['imp_avg']<6000].index:\n",
    "    \n",
    "    if not feature_name.count('mean'):\n",
    "        continue\n",
    "\n",
    "    if feature_name.count('raw'):\n",
    "        from_dir = 'raw_use'\n",
    "        to_dir = 'raw_trush'\n",
    "#         from_dir = 'raw_trush'\n",
    "#         to_dir = 'raw_use'\n",
    "    else:\n",
    "        from_dir = 'org_use'\n",
    "        to_dir = 'org_trush'\n",
    "#         from_dir = 'org_trush'\n",
    "#         to_dir = 'org_use'\n",
    "#     from_dir = 'product_feature'\n",
    "#     from_dir = 'kernel'\n",
    "#     from_dir = 'check_trush'\n",
    "#     from_dir = 'raw_trush'\n",
    "#     from_dir = 'raw_use'\n",
    "#     from_dir = 'org_trush'\n",
    "#     from_dir = 'useless'\n",
    "    from_dir = '../submit/re_sub'\n",
    "#     from_dir = '../submit/add_feature/'\n",
    "#     from_dir = 'org_use'\n",
    "#     from_dir = 'valid_use'\n",
    "#     from_dir = 'valid_trush'\n",
    "#     to_dir = '../submit/escape'\n",
    "#     to_dir = '../submit/add_feature'\n",
    "#     to_dir = '../submit/add_feature'\n",
    "#     to_dir = 'valid_trush'\n",
    "#     to_dir = 'valid_use'\n",
    "#     to_dir = 'raw_trush'\n",
    "#     to_dir = 'check_trush'\n",
    "#     to_dir = 'org_use'\n",
    "#     to_dir = 'useless'\n",
    "#     to_dir = 'valid'\n",
    "#     to_dir = 'kernel'\n",
    "    try:\n",
    "#         print(feature_name)\n",
    "        move_feature([feature_name], from_dir, to_dir)\n",
    "        cnt+=1\n",
    "    except FileNotFoundError:\n",
    "        print(feature_name)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Diff Features: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-29 06:04:52,252 func.utils 111 [INFO]    [<module>] * EXP: dataset new_set (590540, 1682) lr 0.01  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.905843\n",
      "[400]\tvalid_0's auc: 0.919909\n",
      "[600]\tvalid_0's auc: 0.926924\n",
      "[800]\tvalid_0's auc: 0.930362\n"
     ]
    }
   ],
   "source": [
    "is_submit = [True, False][0]\n",
    "has_dec = [True, False][0]\n",
    "n_splits = 6\n",
    "set_type = 'new_set'\n",
    "\n",
    "valid_paths_train = paths_train_feature[:]\n",
    "valid_paths_test  = paths_test_feature[:]\n",
    "\n",
    "#========================================================================\n",
    "# pathの存在チェック。なぜかたびたびFileNotFoundErrorが起きるので,,,\n",
    "#========================================================================\n",
    "remove_paths = []\n",
    "for trn_path, tes_path in zip(valid_paths_train, valid_paths_test):\n",
    "    if os.path.exists(trn_path) and os.path.exists(tes_path):\n",
    "        pass\n",
    "    else:\n",
    "        remove_paths.append(trn_path)\n",
    "        remove_paths.append(tes_path)\n",
    "for path in remove_paths:\n",
    "    if path.count('train'):\n",
    "        valid_paths_train.remove(path)\n",
    "        print(f'remove {path}')\n",
    "    elif path.count('test'):\n",
    "        valid_paths_test.remove(path)\n",
    "        print(f'remove {path}')\n",
    "\n",
    "if len(valid_paths_train):\n",
    "    df_feat_train = parallel_load_data(valid_paths_train)\n",
    "    df_feat_test  = parallel_load_data(valid_paths_test)\n",
    "    \n",
    "    col_drops = eval_check_feature(df_feat_train, df_feat_test)\n",
    "    \n",
    "    tmp_train = df_train.join(df_feat_train)\n",
    "    tmp_test = df_test.join(df_feat_test)\n",
    "else:\n",
    "    tmp_train = df_train\n",
    "    tmp_test = df_test\n",
    "\n",
    "#========================================================================\n",
    "# Train Test で片方に存在しないFeatureを除外\n",
    "#========================================================================\n",
    "diff_cols = list(set(tmp_train.columns) - set(tmp_test.columns))\n",
    "\n",
    "for col in list(set(diff_cols)):\n",
    "    from_dir = 'valid'\n",
    "    to_dir = 'valid_trush'\n",
    "    move_feature([col], from_dir, to_dir)\n",
    "tmp_train.drop(diff_cols, axis=1, inplace=True)\n",
    "print(f\"  * Diff Features: {len(diff_cols)}\")\n",
    "\n",
    "# same_user_path = '../output/same_user_pattern/20190901_user_ids_share.csv'\n",
    "# same_user_path = '../output/same_user_pattern/0902__same_user_id__card_addr_pemail_M.csv'\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "tmp_train[COLUMN_GROUP] = group\n",
    "\n",
    "# 2017-12抜いてみる\n",
    "# if not has_dec:\n",
    "#     tmp_train = tmp_train[tmp_train[COLUMN_GROUP]!='2017-12']\n",
    "#     Y = Y.loc[tmp_train.index]\n",
    "#     n_splits = 5\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Features elimination \n",
    "#==============================================================\n",
    "# from scipy.stats import ks_2samp\n",
    "# features_check = []\n",
    "# columns_to_check = set(list(tmp_train)).difference(COLUMNS_IGNORE)\n",
    "# for i in columns_to_check:\n",
    "#     features_check.append(ks_2samp(tmp_test[i], tmp_train[i])[1])\n",
    "\n",
    "# features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "# features_discard = list(features_check[features_check==0].index)\n",
    "# print(features_discard)\n",
    "# tmp_train.drop(features_discard, axis=1, inplace=True)\n",
    "# tmp_test.drop(features_discard, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "model_type = \"lgb\"\n",
    "params = {\n",
    "#     'n_jobs': 60,\n",
    "    'n_jobs': 96,\n",
    "#     'n_jobs': 84,\n",
    "#     'n_jobs': 48,\n",
    "#     'n_jobs': 36,\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 2**8-1,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree' : 0.10,\n",
    "    'lambda_l1' : 0.1,\n",
    "    'lambda_l2' : 1.0,\n",
    "    'learning_rate' : 0.1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"seed\": 1208,\n",
    "    \"bagging_seed\": 1208,\n",
    "    \"feature_fraction_seed\": 1208,\n",
    "    \"drop_seed\": 1208,\n",
    "    'n_splits': n_splits,\n",
    "    'metric': 'auc',\n",
    "    'model_type': model_type,\n",
    "    'fold': ['stratified', 'group'][1],\n",
    "}\n",
    "if is_submit:\n",
    "    params['learning_rate'] = 0.01\n",
    "#     params['learning_rate'] = 0.05\n",
    "    params[\"early_stopping_rounds\"] = 100\n",
    "\n",
    "logger.info(f\"* EXP: dataset {set_type} {tmp_train.shape} lr {params['learning_rate']} \")\n",
    "\n",
    "list_result_feim = eval_train(\n",
    "    logger,\n",
    "    tmp_train,\n",
    "    Y,\n",
    "    tmp_test,\n",
    "    COLUMN_GROUP,\n",
    "    model_type,\n",
    "    params,\n",
    "    is_adv=[True, False][1],\n",
    "    is_viz=[True, False][0],\n",
    ")\n",
    "feim = list_result_feim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
