{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, datetime, math\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "MAKE_MODEL_TEST = False\n",
    "TARGET = 'isFraud'\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "lgb_params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.01,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':80000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold:',fold_)\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n",
    "            \n",
    "        print(len(tr_x),len(vl_x))\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            vl_data = lgb.Dataset(P, label=P_y) \n",
    "        else:\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n",
    "\n",
    "        estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "        pp_p = estimator.predict(P)\n",
    "        predictions += pp_p/NFOLDS\n",
    "\n",
    "        if LOCAL_TEST:\n",
    "            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "            print(feature_imp)\n",
    "        \n",
    "        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n",
    "        gc.collect()\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------\n",
    "\n",
    "def make_test_predictions(tr_df, tt_df, target, lgb_params, NFOLDS=2):\n",
    "    \n",
    "    new_columns = set(list(train_df)).difference(base_columns + remove_features)\n",
    "    features_columns = base_columns + list(new_columns)\n",
    "    \n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    X,y = tr_df[features_columns], tr_df[target]    \n",
    "    P,P_y = tt_df[features_columns], tt_df[target]  \n",
    "\n",
    "    for col in list(X):\n",
    "        if X[col].dtype=='O':\n",
    "            X[col] = X[col].fillna('unseen_before_label')\n",
    "            P[col] = P[col].fillna('unseen_before_label')\n",
    "\n",
    "            X[col] = train_df[col].astype(str)\n",
    "            P[col] = test_df[col].astype(str)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            le.fit(list(X[col])+list(P[col]))\n",
    "            X[col] = le.transform(X[col])\n",
    "            P[col]  = le.transform(P[col])\n",
    "\n",
    "            X[col] = X[col].astype('category')\n",
    "            P[col] = P[col].astype('category')\n",
    "        \n",
    "    tt_df = tt_df[['TransactionID',target]]    \n",
    "    predictions = np.zeros(len(tt_df))\n",
    "\n",
    "    tr_data = lgb.Dataset(X, label=y)\n",
    "    vl_data = lgb.Dataset(P, label=P_y) \n",
    "    estimator = lgb.train(\n",
    "            lgb_params,\n",
    "            tr_data,\n",
    "            valid_sets = [tr_data, vl_data],\n",
    "            verbose_eval = 200,\n",
    "        )   \n",
    "        \n",
    "    pp_p = estimator.predict(P)\n",
    "    predictions += pp_p/NFOLDS\n",
    "\n",
    "    if LOCAL_TEST:\n",
    "        feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n",
    "        print(feature_imp)\n",
    "        \n",
    "    tt_df['prediction'] = predictions\n",
    "    \n",
    "    return tt_df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_normalization(dt_df, periods, columns):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            dt_df[col] = dt_df[col].astype(float)  \n",
    "\n",
    "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
    "            temp_min.index = temp_min[period].values\n",
    "            temp_min = temp_min['min'].to_dict()\n",
    "\n",
    "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
    "            temp_max.index = temp_max[period].values\n",
    "            temp_max = temp_max['max'].to_dict()\n",
    "\n",
    "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
    "            temp_mean.index = temp_mean[period].values\n",
    "            temp_mean = temp_mean['mean'].to_dict()\n",
    "\n",
    "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
    "            temp_std.index = temp_std[period].values\n",
    "            temp_std = temp_std['std'].to_dict()\n",
    "\n",
    "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
    "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
    "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
    "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
    "\n",
    "            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])/(dt_df['temp_max']-dt_df['temp_min'])\n",
    "            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])/(dt_df['temp_std'])\n",
    "            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n",
    "    return dt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(train_df, test_df, columns, self_encoding=False):\n",
    "    for col in columns:\n",
    "        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n",
    "        if self_encoding:\n",
    "            train_df[col] = train_df[col].map(fq_encode)\n",
    "            test_df[col]  = test_df[col].map(fq_encode)            \n",
    "        else:\n",
    "            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n",
    "                                 with_proportions=True, only_proportions=False):\n",
    "    for period in periods:\n",
    "        for col in columns:\n",
    "            new_col = col +'_'+ period\n",
    "            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n",
    "            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n",
    "\n",
    "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
    "            fq_encode = temp_df[new_col].value_counts().to_dict()\n",
    "\n",
    "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
    "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
    "            \n",
    "            if only_proportions:\n",
    "                train_df[new_col] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col]  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "            if with_proportions:\n",
    "                train_df[new_col+'_proportions'] = train_df[new_col]/train_df[period+'_total']\n",
    "                test_df[new_col+'_proportions']  = test_df[new_col]/test_df[period+'_total']\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def uid_aggregation_and_normalization(train_df, test_df, main_columns, uids, aggregations):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            \n",
    "            new_norm_col_name = col+'_'+main_column+'_std_norm'\n",
    "            norm_cols = []\n",
    "            \n",
    "            for agg_type in aggregations:\n",
    "                new_col_name = col+'_'+main_column+'_'+agg_type\n",
    "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
    "                norm_cols.append(new_col_name)\n",
    "            \n",
    "            train_df[new_norm_col_name] = (train_df[main_column]-train_df[norm_cols[0]])/train_df[norm_cols[1]]\n",
    "            test_df[new_norm_col_name]  = (test_df[main_column]-test_df[norm_cols[0]])/test_df[norm_cols[1]]          \n",
    "            \n",
    "            del train_df[norm_cols[0]], train_df[norm_cols[1]]\n",
    "            del test_df[norm_cols[0]], test_df[norm_cols[1]]\n",
    "                                              \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cor_and_remove(train_df, test_df, i_cols, new_columns, remove=False):\n",
    "    # Check correllation\n",
    "    print('Correlations','#'*10)\n",
    "    for col in new_columns:\n",
    "        cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
    "        print(col, cor_cof)\n",
    "\n",
    "    if remove:\n",
    "        print('#'*10)\n",
    "        print('Best options:')\n",
    "        best_fe_columns = []\n",
    "        for main_col in i_cols:\n",
    "            best_option = ''\n",
    "            best_cof = 0\n",
    "            for col in new_columns:\n",
    "                if main_col in col:\n",
    "                    cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
    "                    cor_cof = (cor_cof**2)**0.5\n",
    "                    if cor_cof>best_cof:\n",
    "                        best_cof = cor_cof\n",
    "                        best_option = col\n",
    "\n",
    "            print(main_col, best_option, best_cof)            \n",
    "            best_fe_columns.append(best_option)\n",
    "\n",
    "        for col in new_columns:\n",
    "            if col not in best_fe_columns:\n",
    "                del train_df[col], test_df[col]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Shape control: (590540, 394) (506691, 393)\n"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "train_df = pd.read_csv('../input/train_transaction.csv')\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    \n",
    "    # Convert TransactionDT to \"Month\" time-period. \n",
    "    # We will also drop penultimate block \n",
    "    # to \"simulate\" test set values difference\n",
    "    train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n",
    "    test_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n",
    "    train_df = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n",
    "    \n",
    "    train_identity = pd.read_csv('../input/train_identity.csv')\n",
    "    test_identity  = train_identity[train_identity['TransactionID'].isin(\n",
    "                                    test_df['TransactionID'])].reset_index(drop=True)\n",
    "    train_identity = train_identity[train_identity['TransactionID'].isin(\n",
    "                                    train_df['TransactionID'])].reset_index(drop=True)\n",
    "    del train_df['DT_M'], test_df['DT_M']\n",
    "    \n",
    "else:\n",
    "    test_df = pd.read_csv('../input/test_transaction.csv')\n",
    "    train_identity = pd.read_csv('../input/train_identity.csv')\n",
    "    test_identity = pd.read_csv('../input/test_identity.csv')\n",
    "    \n",
    "print('Shape control:', train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### All features columns\n",
    "#################################################################################\n",
    "## Main Data\n",
    "# 'TransactionID',                     -> This is pure noise, we cannot use this column as feature\n",
    "# 'isFraud',                           -> Our Target\n",
    "# 'TransactionDT',                     -> Time from reference time point. VERY valuable column\n",
    "# 'TransactionAmt',                    -> Many unique values and has to be combined with other columns\n",
    "#                                         The best score boost should come from \n",
    "#                                         TransactionDT->TransactionAmt combination\n",
    "# 'ProductCD',                         -> 100% categorical feature options to use:\n",
    "#                                         Frequency encoding/Target encoding/\n",
    "#                                         Combinations with other columns/Model categorical feature\n",
    "# 'card1' - 'card6',                   -> Categorical features with information about Client\n",
    "# 'addr1' - 'addr2',                   -> add2 - Country / addr1 - subzone\n",
    "# 'dist1' - 'dist2',                   -> dist2 - Country distance / dist1 - local distance from merchant\n",
    "# 'P_emaildomain' - 'R_emaildomain',   -> Categorical feature. It's possible to make \n",
    "#                                         subgroup feature from it or general group\n",
    "# 'C1' - 'C14'                         -> Counts. Should be numerical features (all ints?)\n",
    "# 'D1' - 'D15'                         -> Timedeltas - minimal value will be same for each month and day\n",
    "#                                         but maximum and mean values will grow over time \n",
    "# 'M1' - 'M9'\n",
    "# 'V1' - 'V339'\n",
    "\n",
    "## Identity Data\n",
    "# 'TransactionID'\n",
    "# 'id_01' - 'id_38'\n",
    "# 'DeviceType',\n",
    "# 'DeviceInfo'\n",
    "\n",
    "# Add list of feature that we will\n",
    "# remove later from final features list\n",
    "remove_features = [\n",
    "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
    "    TARGET,\n",
    "    ]\n",
    "\n",
    "base_columns = [col for col in list(train_df) if col not in remove_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's make baseline model \n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TransactionDT\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "# Let's add temporary \"time variables\" for aggregations\n",
    "# and add normal \"time variables\"\n",
    "for df in [train_df, test_df]:\n",
    "    \n",
    "    # Temporary variables for aggregation\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n",
    "    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
    "    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n",
    "    \n",
    "    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n",
    "    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n",
    "    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n",
    "        \n",
    "    # Possible solo feature\n",
    "    df['is_december'] = df['DT'].dt.month\n",
    "    df['is_december'] = (df['is_december']==12).astype(np.int8)\n",
    "\n",
    "    # Holidays\n",
    "    df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n",
    "\n",
    "# Remove temporary features from final list\n",
    "remove_features += ['DT','DT_M','DT_W','DT_D','DT_hour','DT_day_week','DT_day_month']\n",
    "    \n",
    "# Total transactions per timeblock\n",
    "for col in ['DT_M','DT_W','DT_D']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()\n",
    "            \n",
    "    train_df[col+'_total'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_total']  = test_df[col].map(fq_encode)\n",
    "    \n",
    "    # We can't use it as solo feature\n",
    "    remove_features.append(col+'_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Start with FE\n",
    "# Before we start with FE I would like to do\n",
    "# few things\n",
    "# 1. Find and reset \"outliers\" for card1 and card2\n",
    "# 2. Create \"Virtual\" client uID\n",
    "# Reset values for \"noise\" card1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare cards 5993\n",
      "No intersection in Train 10396\n",
      "Intersection in Train 580144\n",
      "####################\n",
      "No intersection in Train card2 5012\n",
      "Intersection in Train card2 585528\n",
      "####################\n",
      "No intersection in Train card3 47\n",
      "Intersection in Train card3 590493\n",
      "####################\n",
      "No intersection in Train card4 0\n",
      "Intersection in Train card4 590540\n",
      "####################\n",
      "No intersection in Train card5 7279\n",
      "Intersection in Train card5 583261\n",
      "####################\n",
      "No intersection in Train card6 30\n",
      "Intersection in Train card6 590510\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "########################### Card columns \"outliers\"\n",
    "for col in ['card1']: \n",
    "    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    valid_card = valid_card[col].value_counts()\n",
    "    valid_card_std = valid_card.values.std()\n",
    "\n",
    "    invalid_cards = valid_card[valid_card<=2]\n",
    "    print('Rare cards',len(invalid_cards))\n",
    "\n",
    "    valid_card = valid_card[valid_card>2]\n",
    "    valid_card = list(valid_card.index)\n",
    "\n",
    "    print('No intersection in Train', len(train_df[~train_df[col].isin(test_df[col])]))\n",
    "    print('Intersection in Train', len(train_df[train_df[col].isin(test_df[col])]))\n",
    "    \n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "\n",
    "    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n",
    "    print('#'*20)\n",
    "\n",
    "for col in ['card2','card3','card4','card5','card6',]: \n",
    "    print('No intersection in Train', col, len(train_df[~train_df[col].isin(test_df[col])]))\n",
    "    print('Intersection in Train', col, len(train_df[train_df[col].isin(test_df[col])]))\n",
    "    \n",
    "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
    "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
    "    print('#'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Most common uIds:\n",
      "########## uid\n",
      "7919.0_194.0     14891\n",
      "9500.0_321.0     14112\n",
      "15885.0_545.0    10332\n",
      "17188.0_321.0    10312\n",
      "15066.0_170.0     7918\n",
      "12695.0_490.0     7079\n",
      "6019.0_583.0      6766\n",
      "12544.0_321.0     6760\n",
      "2803.0_100.0      6126\n",
      "7585.0_553.0      5325\n",
      "Name: uid, dtype: int64\n",
      "########## uid2\n",
      "9500.0_321.0_150.0_226.0     14112\n",
      "15885.0_545.0_185.0_138.0    10332\n",
      "17188.0_321.0_150.0_226.0    10312\n",
      "7919.0_194.0_150.0_166.0      8844\n",
      "15066.0_170.0_150.0_102.0     7918\n",
      "12695.0_490.0_150.0_226.0     7079\n",
      "6019.0_583.0_150.0_226.0      6766\n",
      "12544.0_321.0_150.0_226.0     6760\n",
      "2803.0_100.0_150.0_226.0      6126\n",
      "7919.0_194.0_150.0_nan        6047\n",
      "Name: uid2, dtype: int64\n",
      "########## uid3\n",
      "15885.0_545.0_185.0_138.0_nan_nan       9900\n",
      "17188.0_321.0_150.0_226.0_299.0_87.0    5862\n",
      "12695.0_490.0_150.0_226.0_325.0_87.0    5766\n",
      "9500.0_321.0_150.0_226.0_204.0_87.0     4647\n",
      "3154.0_408.0_185.0_224.0_nan_nan        4398\n",
      "12839.0_321.0_150.0_226.0_264.0_87.0    3538\n",
      "16132.0_111.0_150.0_226.0_299.0_87.0    3523\n",
      "15497.0_490.0_150.0_226.0_299.0_87.0    3419\n",
      "9500.0_321.0_150.0_226.0_272.0_87.0     2715\n",
      "5812.0_408.0_185.0_224.0_nan_nan        2639\n",
      "Name: uid3, dtype: int64\n",
      "########## uid4\n",
      "15885.0_545.0_185.0_138.0_nan_nan_hotmail.com     4002\n",
      "15885.0_545.0_185.0_138.0_nan_nan_gmail.com       3830\n",
      "17188.0_321.0_150.0_226.0_299.0_87.0_gmail.com    2235\n",
      "12695.0_490.0_150.0_226.0_325.0_87.0_gmail.com    2045\n",
      "9500.0_321.0_150.0_226.0_204.0_87.0_gmail.com     1947\n",
      "3154.0_408.0_185.0_224.0_nan_nan_hotmail.com      1890\n",
      "3154.0_408.0_185.0_224.0_nan_nan_gmail.com        1537\n",
      "12839.0_321.0_150.0_226.0_264.0_87.0_gmail.com    1473\n",
      "15775.0_481.0_150.0_102.0_330.0_87.0_nan          1453\n",
      "15497.0_490.0_150.0_226.0_299.0_87.0_gmail.com    1383\n",
      "Name: uid4, dtype: int64\n",
      "########## uid5\n",
      "12695.0_490.0_150.0_226.0_325.0_87.0_nan         5446\n",
      "17188.0_321.0_150.0_226.0_299.0_87.0_nan         5322\n",
      "9500.0_321.0_150.0_226.0_204.0_87.0_nan          4403\n",
      "15885.0_545.0_185.0_138.0_nan_nan_hotmail.com    4002\n",
      "15885.0_545.0_185.0_138.0_nan_nan_gmail.com      3830\n",
      "12839.0_321.0_150.0_226.0_264.0_87.0_nan         3365\n",
      "16132.0_111.0_150.0_226.0_299.0_87.0_nan         3212\n",
      "15497.0_490.0_150.0_226.0_299.0_87.0_nan         3027\n",
      "9500.0_321.0_150.0_226.0_272.0_87.0_nan          2601\n",
      "7664.0_490.0_150.0_226.0_264.0_87.0_nan          2396\n",
      "Name: uid5, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "########################### Client Virtual ID\n",
    "# Let's add some kind of client uID based on cardID and addr columns\n",
    "# The value will be very specific for each client so we need to remove it\n",
    "# from final features. But we can use it for aggregations.\n",
    "train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n",
    "\n",
    "train_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "train_df['uid4'] = train_df['uid3'].astype(str)+'_'+train_df['P_emaildomain'].astype(str)\n",
    "test_df['uid4'] = test_df['uid3'].astype(str)+'_'+test_df['P_emaildomain'].astype(str)\n",
    "\n",
    "train_df['uid5'] = train_df['uid3'].astype(str)+'_'+train_df['R_emaildomain'].astype(str)\n",
    "test_df['uid5'] = test_df['uid3'].astype(str)+'_'+test_df['R_emaildomain'].astype(str)\n",
    "\n",
    "# Add values remove list\n",
    "new_columns = ['uid','uid2','uid3','uid4','uid5']\n",
    "remove_features += new_columns\n",
    "\n",
    "print('#'*10)\n",
    "print('Most common uIds:')\n",
    "for col in new_columns:\n",
    "    print('#'*10, col)\n",
    "    print(train_df[col].value_counts()[:10])\n",
    "\n",
    "# Do Global frequency encoding \n",
    "i_cols = ['card1','card2','card3','card5'] + new_columns\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### card3/card5 most common hour \n",
    "# card3 or card5 is a bank country?\n",
    "# can we find:\n",
    "# - the most popular Transaction Hour\n",
    "# - the most popular Week Day\n",
    "# and then find distance from it\n",
    "\n",
    "# Prepare bank type feature\n",
    "for df in [train_df, test_df]:\n",
    "    df['bank_type'] = df['card3'].astype(str) +'_'+ df['card5'].astype(str)\n",
    "remove_features.append('bank_type') \n",
    "\n",
    "encoding_mean = {\n",
    "    1: ['DT_D','DT_hour','_hour_dist','DT_hour_mean'],\n",
    "    2: ['DT_W','DT_day_week','_week_day_dist','DT_day_week_mean'],\n",
    "    3: ['DT_M','DT_day_month','_month_day_dist','DT_day_month_mean'],\n",
    "    }\n",
    "\n",
    "encoding_best = {\n",
    "    1: ['DT_D','DT_hour','_hour_dist_best','DT_hour_best'],\n",
    "    2: ['DT_W','DT_day_week','_week_day_dist_best','DT_day_week_best'],\n",
    "    3: ['DT_M','DT_day_month','_month_day_dist_best','DT_day_month_best'],   \n",
    "    }\n",
    "\n",
    "# Some ugly code here (even worse than in other parts)\n",
    "for col in ['card3','card5','bank_type']:\n",
    "    for df in [train_df, test_df]:\n",
    "        for encode in encoding_mean:\n",
    "            encode = encoding_mean[encode].copy()\n",
    "            new_col = col + '_' + encode[0] + encode[2]\n",
    "            df[new_col] = df[col].astype(str) +'_'+ df[encode[0]].astype(str)\n",
    "\n",
    "            temp_dict = df.groupby([new_col])[encode[1]].agg(['mean']).reset_index().rename(\n",
    "                                                                    columns={'mean': encode[3]})\n",
    "            temp_dict.index = temp_dict[new_col].values\n",
    "            temp_dict = temp_dict[encode[3]].to_dict()\n",
    "            df[new_col] = df[encode[1]] - df[new_col].map(temp_dict)\n",
    "\n",
    "        for encode in encoding_best:\n",
    "            encode = encoding_best[encode].copy()\n",
    "            new_col = col + '_' + encode[0] + encode[2]\n",
    "            df[new_col] = df[col].astype(str) +'_'+ df[encode[0]].astype(str)\n",
    "            temp_dict = df.groupby([col,encode[0],encode[1]])[encode[1]].agg(['count']).reset_index().rename(\n",
    "                                                                    columns={'count': encode[3]})\n",
    "\n",
    "            temp_dict.sort_values(by=[col,encode[0],encode[3]], inplace=True)\n",
    "            temp_dict = temp_dict.drop_duplicates(subset=[col,encode[0]], keep='last')\n",
    "            temp_dict[new_col] = temp_dict[col].astype(str) +'_'+ temp_dict[encode[0]].astype(str)\n",
    "            temp_dict.index = temp_dict[new_col].values\n",
    "            temp_dict = temp_dict[encode[1]].to_dict()\n",
    "            df[new_col] = df[encode[1]] - df[new_col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### bank_type\n",
    "# Tracking nomal activity\n",
    "# by doing timeblock frequency encoding\n",
    "i_cols = ['bank_type'] #['uid','uid2','uid3','uid4','uid5','bank_type']\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "\n",
    "# We have few options to encode it here:\n",
    "# - Just count transactions\n",
    "# (but some timblocks have more transactions than others)\n",
    "# - Devide to total transactions per timeblock (proportions)\n",
    "# - Use both\n",
    "# - Use only proportions\n",
    "train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols, \n",
    "                                 with_proportions=False, only_proportions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### D Columns\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "periods = ['DT_D']\n",
    "\n",
    "temp_df = pd.concat([train_df[['TransactionDT']+i_cols+periods], test_df[['TransactionDT']+i_cols+periods]])\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        for df in [temp_df]:\n",
    "            df.set_index(period)[col].plot(style='.', title=col, figsize=(15, 3))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### D Columns\n",
    "# From columns description we know that\n",
    "# D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "# 1. I can't imagine normal negative timedelta values (Let's clip Values)\n",
    "# 2. Normalize (Min-Max, Standard score) All D columns, except D1,D2,D9\n",
    "# 3. Do some aggregations based on uIDs\n",
    "# 4. Freaquency encoding\n",
    "# 5. D1,D2 are clipped by max train_df values (let's scale it)\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "uids = ['uid','uid2','uid3','uid4','uid5','bank_type']\n",
    "aggregations = ['mean','std']\n",
    "\n",
    "####### uIDs aggregations\n",
    "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    "\n",
    "####### Cleaning Neagtive values and columns transformations\n",
    "for df in [train_df, test_df]:\n",
    "\n",
    "    for col in i_cols:\n",
    "        df[col] = df[col].clip(0) \n",
    "    \n",
    "    # Lets transform D8 and D9 column\n",
    "    # As we almost sure it has connection with hours\n",
    "    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n",
    "    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n",
    "    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n",
    "    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n",
    "    df['D8'] = df['D8'].fillna(-1).astype(int)\n",
    "\n",
    "####### Values Normalization\n",
    "i_cols.remove('D1')\n",
    "i_cols.remove('D2')\n",
    "i_cols.remove('D9')\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "for df in [train_df, test_df]:\n",
    "    df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "for col in ['D1','D2']:\n",
    "    for df in [train_df, test_df]:\n",
    "        df[col+'_scaled'] = df[col]/train_df[col].max()\n",
    "        \n",
    "####### Global Self frequency encoding\n",
    "# self_encoding=True because \n",
    "# we don't need original values anymore\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################### TransactionAmt\n",
    "i_cols = ['TransactionAmt']\n",
    "periods = ['DT_D']\n",
    "\n",
    "temp_df = pd.concat([train_df[['TransactionDT']+i_cols+periods], test_df[['TransactionDT']+i_cols+periods]])\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        for df in [temp_df]:\n",
    "            df.set_index(period)[col].plot(style='.', title=col, figsize=(15, 3))\n",
    "            plt.show()\n",
    "\n",
    "# Clip Values\n",
    "train_df['TransactionAmt'] = train_df['TransactionAmt'].clip(0,5000)\n",
    "test_df['TransactionAmt']  = test_df['TransactionAmt'].clip(0,5000)\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values   \n",
    "train_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
    "test_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise\n",
    "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "# (even if features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['TransactionAmt']\n",
    "uids = ['card1','card2','card3','card5','uid','uid2','uid3','uid4','uid5','bank_type']\n",
    "aggregations = ['mean','std']\n",
    "\n",
    "# uIDs aggregations\n",
    "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
    " \n",
    "# TransactionAmt Normalization\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "for df in [train_df, test_df]:\n",
    "    df = values_normalization(df, periods, i_cols)\n",
    "\n",
    "# Product type\n",
    "train_df['product_type'] = train_df['ProductCD'].astype(str)+'_'+train_df['TransactionAmt'].astype(str)\n",
    "test_df['product_type'] = test_df['ProductCD'].astype(str)+'_'+test_df['TransactionAmt'].astype(str)\n",
    "\n",
    "i_cols = ['product_type']\n",
    "periods = ['DT_D','DT_W','DT_M']\n",
    "train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols, \n",
    "                                                 with_proportions=False, only_proportions=True)\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=True)\n",
    "\n",
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "train_df['TransactionAmt'] = np.log1p(train_df['TransactionAmt'])\n",
    "test_df['TransactionAmt'] = np.log1p(test_df['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### C Columns\n",
    "i_cols = ['C'+str(i) for i in range(1,15)]\n",
    "\n",
    "####### Global Self frequency encoding\n",
    "# self_encoding=False because \n",
    "# I want to keep original values\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)\n",
    "\n",
    "####### Clip max values\n",
    "for df in [train_df, test_df]:\n",
    "    for col in i_cols:\n",
    "        max_value = train_df[train_df['DT_M']==train_df['DT_M'].max()][col].max()\n",
    "        df[col] = df[col].clip(None,max_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Device info and identity\n",
    "for df in [train_identity, test_identity]:\n",
    "    ########################### Device info\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    ########################### Device info 2\n",
    "    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "    ########################### Browser\n",
    "    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "    \n",
    "########################### Merge Identity columns\n",
    "temp_df = train_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "train_df = pd.concat([train_df,temp_df], axis=1)\n",
    "    \n",
    "temp_df = test_df[['TransactionID']]\n",
    "temp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\n",
    "del temp_df['TransactionID']\n",
    "test_df = pd.concat([test_df,temp_df], axis=1)\n",
    "\n",
    "i_cols = [\n",
    "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
    "          'id_30','id_30_device','id_30_version',\n",
    "          'id_31','id_31_device',\n",
    "          'id_33',\n",
    "         ]\n",
    "\n",
    "####### Global Self frequency encoding\n",
    "# self_encoding=True because \n",
    "# we don't need original values anymore\n",
    "train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### ProductCD and M4 Target mean\n",
    "# As we already have frequency encoded columns\n",
    "# We can have different global transformation on them\n",
    "# Target mean?\n",
    "# We will transform original values as we don't need them\n",
    "# Leakage over folds?\n",
    "# Yes, we will have some,\n",
    "# But in the same time we already have leakage from \n",
    "# V columns and card1->card6 columns\n",
    "# So, no much harm here\n",
    "for col in ['ProductCD','M4']:\n",
    "    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n",
    "                                                        columns={'mean': col+'_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "    train_df[col] = train_df[col].map(temp_dict)\n",
    "    test_df[col]  = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card4\n",
      "card6\n",
      "P_emaildomain\n",
      "R_emaildomain\n",
      "M1\n",
      "M2\n",
      "M3\n",
      "M5\n",
      "M6\n",
      "M7\n",
      "M8\n",
      "M9\n",
      "uid\n",
      "uid2\n",
      "uid3\n",
      "uid4\n",
      "uid5\n",
      "bank_type\n",
      "id_12\n",
      "id_15\n",
      "id_16\n",
      "id_23\n",
      "id_27\n",
      "id_28\n",
      "id_29\n",
      "id_34\n",
      "id_35\n",
      "id_36\n",
      "id_37\n",
      "id_38\n",
      "DeviceType\n"
     ]
    }
   ],
   "source": [
    "########################### Encode Str columns\n",
    "# For all such columns (probably not)\n",
    "# we already did frequency encoding (numeric feature)\n",
    "# so we will use astype('category') here\n",
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test new features\n",
    "if MAKE_MODEL_TEST:\n",
    "    test_predictions = make_test_predictions(train_df, test_df, TARGET, lgb_params)\n",
    "    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Minification\n",
    "train_df.to_pickle('train_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl')\n",
    "\n",
    "remove_features = pd.DataFrame(remove_features, columns=['features_to_remove'])\n",
    "remove_features.to_pickle('remove_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 789)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
