{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func.utils import get_numeric_features, get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename\n",
    "from func.time_utils import date_add_days\n",
    "from func.ml_utils import save_feature, get_cnt_feature, get_dummie_feature, get_label_feature\n",
    "from func.parallel_utils import get_parallel_arg_list\n",
    "from joblib import delayed, Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, 'ProductCD']\n",
    "\n",
    "train_paths = glob('../feature/eda_base/*_train.gz')\n",
    "test_paths = glob('../feature/eda_base/*_test.gz')\n",
    "\n",
    "train_paths = [path for path in train_paths \n",
    "               if path.count(COLUMN_DT) \n",
    "               or path.count(COLUMN_ID)\n",
    "               or path.count('C')\n",
    "               or path.count('D')\n",
    "               or path.count('V')\n",
    "               or path.count('card')\n",
    "               or path.count('addr')\n",
    "               or path.count('domain')\n",
    "               or path.count('Product')\n",
    "              ]\n",
    "test_paths = [path for path in test_paths \n",
    "               if path.count(COLUMN_DT) \n",
    "               or path.count(COLUMN_ID)\n",
    "               or path.count('C')\n",
    "               or path.count('D')\n",
    "               or path.count('V')\n",
    "               or path.count('card')\n",
    "               or path.count('addr')\n",
    "               or path.count('domain')\n",
    "               or path.count('Product')\n",
    "              ]\n",
    "\n",
    "df_train = parallel_load_data(train_paths)\n",
    "df_test = parallel_load_data(test_paths)\n",
    "data = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "if COLUMN_ID in data.columns:\n",
    "    data.set_index(COLUMN_ID, inplace=True)\n",
    "\n",
    "base_train = read_pkl_gzip('../input/base_train.gz').set_index(COLUMN_ID)\n",
    "base_test = read_pkl_gzip('../input/base_test.gz').set_index(COLUMN_ID)\n",
    "base = pd.concat([base_train, base_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "\n",
    "data['datetime'] = data['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x) - datetime.timedelta(seconds = 14400) ))\n",
    "data['datetime'].fillna(datetime.date(2020, 1, 1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Feature \n",
    "#========================================================================\n",
    "cols_C = [col for col in data.columns if col.startswith('C')]\n",
    "cols_D = [col for col in data.columns if col.startswith('D')]\n",
    "cols_V = ['V317', 'V45', 'V87', 'V314', 'V258', 'V282', 'V243', 'V201']\n",
    "cols_F = [col for col in data.columns if col.count('diff') or col.count('ratio')]\n",
    "\n",
    "list_domain = [col for col in data.columns if col.count('domain')]\n",
    "data[list_domain[0]].fillna('#', inplace=True)\n",
    "data[list_domain[0] +'_prefix'] = data[list_domain[0]].apply(lambda x: x.split('.')[0])\n",
    "data[list_domain[1]].fillna('#', inplace=True)\n",
    "data[list_domain[1] +'_prefix'] = data[list_domain[0]].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "list_domain = [col for col in data.columns if col.count('prefix')]\n",
    "list_card = [col for col in data.columns if col.count('card')]\n",
    "list_addr = [col for col in data.columns if col.count('addr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/39 [00:02<01:15,  2.00s/it]\u001b[A\n",
      "  5%|▌         | 2/39 [00:03<01:08,  1.85s/it]\u001b[A\n",
      "  8%|▊         | 3/39 [00:05<01:04,  1.78s/it]\u001b[A\n",
      " 10%|█         | 4/39 [00:06<01:00,  1.74s/it]\u001b[A\n",
      " 13%|█▎        | 5/39 [00:08<00:59,  1.74s/it]\u001b[A\n",
      " 15%|█▌        | 6/39 [00:10<00:58,  1.76s/it]\u001b[A\n",
      " 18%|█▊        | 7/39 [00:12<00:56,  1.78s/it]\u001b[A\n",
      " 21%|██        | 8/39 [00:14<00:56,  1.84s/it]\u001b[A\n",
      " 23%|██▎       | 9/39 [00:16<00:56,  1.87s/it]\u001b[A\n",
      " 26%|██▌       | 10/39 [00:18<00:55,  1.91s/it]\u001b[A\n",
      " 28%|██▊       | 11/39 [00:20<00:55,  1.98s/it]\u001b[A\n",
      " 31%|███       | 12/39 [00:22<00:54,  2.02s/it]\u001b[A\n",
      " 33%|███▎      | 13/39 [00:24<00:54,  2.09s/it]\u001b[A\n",
      " 36%|███▌      | 14/39 [00:26<00:53,  2.14s/it]\u001b[A\n",
      " 38%|███▊      | 15/39 [00:29<00:52,  2.20s/it]\u001b[A\n",
      " 41%|████      | 16/39 [00:31<00:52,  2.27s/it]\u001b[A\n",
      " 44%|████▎     | 17/39 [00:34<00:50,  2.31s/it]\u001b[A\n",
      " 46%|████▌     | 18/39 [00:36<00:49,  2.38s/it]\u001b[A\n",
      " 49%|████▊     | 19/39 [00:39<00:48,  2.44s/it]\u001b[A\n",
      " 51%|█████▏    | 20/39 [00:41<00:46,  2.45s/it]\u001b[A\n",
      " 54%|█████▍    | 21/39 [00:44<00:45,  2.51s/it]\u001b[A\n",
      " 56%|█████▋    | 22/39 [00:46<00:43,  2.57s/it]\u001b[A\n",
      " 59%|█████▉    | 23/39 [00:49<00:41,  2.62s/it]\u001b[A\n",
      " 62%|██████▏   | 24/39 [00:52<00:39,  2.66s/it]\u001b[A\n",
      " 64%|██████▍   | 25/39 [00:55<00:37,  2.71s/it]\u001b[A\n",
      " 67%|██████▋   | 26/39 [00:58<00:35,  2.75s/it]\u001b[A\n",
      " 69%|██████▉   | 27/39 [01:01<00:33,  2.81s/it]\u001b[A\n",
      " 72%|███████▏  | 28/39 [01:04<00:31,  2.88s/it]\u001b[A\n",
      " 74%|███████▍  | 29/39 [01:07<00:29,  2.96s/it]\u001b[A\n",
      " 77%|███████▋  | 30/39 [01:10<00:27,  3.01s/it]\u001b[A\n",
      " 79%|███████▉  | 31/39 [01:13<00:24,  3.06s/it]\u001b[A\n",
      " 82%|████████▏ | 32/39 [01:16<00:21,  3.11s/it]\u001b[A\n",
      " 85%|████████▍ | 33/39 [01:20<00:19,  3.17s/it]\u001b[A\n",
      " 87%|████████▋ | 34/39 [01:23<00:16,  3.21s/it]\u001b[A\n",
      " 90%|████████▉ | 35/39 [01:26<00:13,  3.28s/it]\u001b[A\n",
      " 92%|█████████▏| 36/39 [01:30<00:09,  3.33s/it]\u001b[A\n",
      " 95%|█████████▍| 37/39 [01:33<00:06,  3.39s/it]\u001b[A\n",
      " 97%|█████████▋| 38/39 [01:37<00:03,  3.43s/it]\u001b[A\n",
      "100%|██████████| 39/39 [01:40<00:00,  3.48s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# ProductCDあたりのC\n",
    "#========================================================================\n",
    "cols_feature = cols_C + cols_D + cols_V + cols_F\n",
    "cols_pcd = data['ProductCD'].unique()\n",
    "\n",
    "for col in tqdm(cols_feature):\n",
    "    for pcd in cols_pcd:\n",
    "        feature_name = f'{col}__ProductCD-{pcd}'\n",
    "        data[feature_name] = np.nan\n",
    "        data.loc[data['ProductCD'].isin([pcd]), feature_name] = data.loc[data['ProductCD'].isin([pcd]), col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#========================================================================\n",
    "# 複数カテゴリ組み合わせの各feature deltaを計算\n",
    "#========================================================================\n",
    "from itertools import combinations\n",
    "prefix = 523\n",
    "length = len(df_train)\n",
    "dir_save = 'valid_use'\n",
    "feature = 'datetime'\n",
    "\n",
    "# debug\n",
    "# base_key = ['card1', 'card2']\n",
    "# df = data[list(base_key) + [feature]]\n",
    "# for i in range(1):\n",
    "\n",
    "def parallel_agg(df, base_key):\n",
    "    fname = '-'.join(base_key)\n",
    "    base = df[base_key + [feature]].copy()\n",
    "    \n",
    "    tmp = df[~df[feature].isnull()]\n",
    "    tmp.sort_values(by=feature, inplace=True)\n",
    "    \n",
    "    df_shift_p5 = tmp.groupby(base_key)[feature].shift(5)\n",
    "    df_shift_p4 = tmp.groupby(base_key)[feature].shift(4)\n",
    "    df_shift_p3 = tmp.groupby(base_key)[feature].shift(3)\n",
    "    df_shift_p2 = tmp.groupby(base_key)[feature].shift(2)\n",
    "    df_shift_p1 = tmp.groupby(base_key)[feature].shift(1)\n",
    "    df_shift_m1 = tmp.groupby(base_key)[feature].shift(-1)\n",
    "    df_shift_m2 = tmp.groupby(base_key)[feature].shift(-2)\n",
    "    df_shift_m3 = tmp.groupby(base_key)[feature].shift(-3)\n",
    "    df_shift_m4 = tmp.groupby(base_key)[feature].shift(-4)\n",
    "    df_shift_m5 = tmp.groupby(base_key)[feature].shift(-5)\n",
    "    \n",
    "    p5 = 'shift_p5'\n",
    "    p4 = 'shift_p4'\n",
    "    p3 = 'shift_p3'\n",
    "    p2 = 'shift_p2'\n",
    "    p1 = 'shift_p1'\n",
    "    m1 = 'shift_m1'\n",
    "    m2 = 'shift_m2'\n",
    "    m3 = 'shift_m3'\n",
    "    m4 = 'shift_m4'\n",
    "    m5 = 'shift_m5'\n",
    "    \n",
    "    df_shift_p5.name = p5\n",
    "    df_shift_p4.name = p4\n",
    "    df_shift_p3.name = p3\n",
    "    df_shift_p2.name = p2\n",
    "    df_shift_p1.name = p1\n",
    "    df_shift_m1.name = m1\n",
    "    df_shift_m2.name = m2\n",
    "    df_shift_m3.name = m3\n",
    "    df_shift_m4.name = m4\n",
    "    df_shift_m5.name = m5\n",
    "    \n",
    "    df_shift = pd.concat([\n",
    "        df_shift_p5,\n",
    "        df_shift_p4,\n",
    "        df_shift_p3,\n",
    "        df_shift_p2,\n",
    "        df_shift_p1,\n",
    "        df_shift_m1,\n",
    "        df_shift_m2,\n",
    "        df_shift_m3,\n",
    "        df_shift_m4,\n",
    "        df_shift_m5,\n",
    "    ], axis=1, ignore_index=False)\n",
    "    \n",
    "    cols_shift = [col for col in df_shift.columns if col.count('shift_')]\n",
    "    base = base.join(df_shift[cols_shift])\n",
    "    b0 = feature\n",
    "    \n",
    "    # 過去デルタ\n",
    "    base[f'{fname}_past_{b0}_{p1}_diff'] = base[b0] - base[p1]\n",
    "    base[f'{fname}_past_{b0}_{p2}_diff'] = base[b0] - base[p2]\n",
    "    base[f'{fname}_past_{b0}_{p3}_diff'] = base[b0] - base[p3]\n",
    "    base[f'{fname}_past_{b0}_{p4}_diff'] = base[b0] - base[p4]\n",
    "    base[f'{fname}_past_{b0}_{p5}_diff'] = base[b0] - base[p5]\n",
    "    \n",
    "    base[f'{fname}_past_{p1}_{p2}_diff'] = base[p1] - base[p2]\n",
    "    base[f'{fname}_past_{p1}_{p3}_diff'] = base[p1] - base[p3]\n",
    "    base[f'{fname}_past_{p1}_{p4}_diff'] = base[p1] - base[p4]\n",
    "    base[f'{fname}_past_{p1}_{p5}_diff'] = base[p1] - base[p5]\n",
    "    \n",
    "    # 未来デルタ\n",
    "    base[f'{fname}_future_{b0}_{m1}_diff'] = base[b0] - base[m1]\n",
    "    base[f'{fname}_future_{b0}_{m2}_diff'] = base[b0] - base[m2]\n",
    "    base[f'{fname}_future_{b0}_{m3}_diff'] = base[b0] - base[m3]\n",
    "    base[f'{fname}_future_{b0}_{m4}_diff'] = base[b0] - base[m4]\n",
    "    base[f'{fname}_future_{b0}_{m5}_diff'] = base[b0] - base[m5]\n",
    "    \n",
    "    base[f'{fname}_future_{m1}_{m2}_diff'] = base[m1] - base[m2]\n",
    "    base[f'{fname}_future_{m1}_{m3}_diff'] = base[m1] - base[m3]\n",
    "    base[f'{fname}_future_{m1}_{m4}_diff'] = base[m1] - base[m4]\n",
    "    base[f'{fname}_future_{m1}_{m5}_diff'] = base[m1] - base[m5]\n",
    "    \n",
    "    train = base.iloc[:length]\n",
    "    test = base.iloc[length:]\n",
    "    \n",
    "    cols_save = [col for col in train.columns if col.count('future') or col.count('past')]\n",
    "    \n",
    "    for col in tqdm(cols_save):\n",
    "        train[col] = train[col].map(lambda x: x.days)\n",
    "        test[col] = test[col].map(lambda x: x.days)\n",
    "    \n",
    "#     print(train[cols_save].head())\n",
    "    save_feature(train[cols_save], prefix, dir_save, is_train=True, auto_type=True, list_ignore=COLUMNS_IGNORE)\n",
    "    save_feature(test[cols_save],  prefix, dir_save, is_train=False, auto_type=True, list_ignore=COLUMNS_IGNORE)\n",
    "    \n",
    "\n",
    "combi_card = list(combinations(list_card, 2))\n",
    "list_base_key = combi_card\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "combi_card = list(combinations(list_card, 3))\n",
    "list_base_key = combi_card\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "combi_card = list(combinations(list_card, 4))\n",
    "list_base_key = combi_card\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "combi_card = list(combinations(list_card, 5))\n",
    "list_base_key = combi_card\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "list_base_key = []\n",
    "combi_card = list(combinations(list_card, 2))\n",
    "for domain in list_domain:\n",
    "    for card in combi_card:\n",
    "        list_base_key.append([domain] + list(card))\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "list_base_key = []\n",
    "combi_card = list(combinations(list_card, 3))\n",
    "for domain in list_domain:\n",
    "    for card in combi_card:\n",
    "        list_base_key.append([domain] + list(card))\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "list_base_key = []\n",
    "combi_card = list(combinations(list_card, 4))\n",
    "for domain in list_domain:\n",
    "    for card in combi_card:\n",
    "        list_base_key.append([domain] + list(card))\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])\n",
    "\n",
    "list_base_key = []\n",
    "combi_card = list(combinations(list_card, 5))\n",
    "for domain in list_domain:\n",
    "    for card in combi_card:\n",
    "        list_base_key.append([domain] + list(card))\n",
    "Parallel(60)([delayed(parallel_agg)(data[list(base_key) + [feature]], list(base_key)) for base_key in list_base_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
