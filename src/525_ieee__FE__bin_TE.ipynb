{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "from func.utils import get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename\n",
    "from func.ml_utils import save_feature, get_cnt_feature, get_dummie_feature, get_label_feature\n",
    "from kaggle_utils import reduce_mem_usage, move_feature\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, 'is_train', 'date']\n",
    "\n",
    "def filter_feature(path):\n",
    "    if path.count('fill_') or path.count('bin') or path.count(COLUMN_ID) or path.count(COLUMN_TARGET):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "paths_train = glob('../feature/eda_base/*_train.gz')\n",
    "paths_train = [path for path in paths_train if filter_feature(path) ]\n",
    "df_train = parallel_load_data(paths_train)\n",
    "\n",
    "paths_test = glob('../feature/eda_base/*_test.gz')\n",
    "paths_test = [path for path in paths_test if filter_feature(path) ]\n",
    "df_test = parallel_load_data(paths_test)\n",
    "\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "COLUMN_GROUP = 'DT-M'\n",
    "df_train[COLUMN_GROUP] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     463708\n",
       "#        89164\n",
       "2.0      30832\n",
       "3.0       3761\n",
       "4.0       1033\n",
       "0.0        694\n",
       "5.0        480\n",
       "6.0        205\n",
       "7.0        148\n",
       "8.0        147\n",
       "10.0        80\n",
       "9.0         75\n",
       "11.0        54\n",
       "12.0        34\n",
       "19.0        30\n",
       "13.0        25\n",
       "14.0        14\n",
       "15.0        14\n",
       "16.0        10\n",
       "17.0         8\n",
       "22.0         7\n",
       "18.0         5\n",
       "20.0         2\n",
       "21.0         2\n",
       "30.0         1\n",
       "24.0         1\n",
       "23.0         1\n",
       "26.0         1\n",
       "27.0         1\n",
       "28.0         1\n",
       "29.0         1\n",
       "25.0         1\n",
       "Name: bin__V87, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['bin__V87'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Judgeのルールはどう作るか？bearさんが共有してくれたのも使うが、自分でも考えてみる.\n",
    "自分は条件のかけ算だと思っている。しかしこれだとツリーとやってること同じかも？\n",
    "別の期間での各カテゴリのFraud Probをかけ合わせる。また、閾値をもうけてカウントする.\n",
    "Cとかはその怪しいトリガーのカウントではないのか？\n",
    "直近で同じカテゴリを含むトランザクションがFraudしたか、は特徴にならないな。Privateでは使えないから\n",
    "Stripeとか見るか、\n",
    "\n",
    "1. まずはProduct別にcard, addr, domain, device, Amt, (D, C, V, )のTEを行う\n",
    "\"\"\"\n",
    "\n",
    "df_train['cents'] = np.round( df_train['TransactionAmt'] - np.floor(df_train['TransactionAmt']),2 )\n",
    "\n",
    "list_domain = [col for col in df_train.columns if col.count('domain')]\n",
    "df_train[list_domain[0]].fillna('#', inplace=True)\n",
    "df_train[list_domain[0] +'_prefix'] = df_train[list_domain[0]].apply(lambda x: x.split('.')[0])\n",
    "df_train[list_domain[1]].fillna('#', inplace=True)\n",
    "df_train[list_domain[1] +'_prefix'] = df_train[list_domain[0]].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "df_test['cents'] = np.round( df_test['TransactionAmt'] - np.floor(df_test['TransactionAmt']),2 )\n",
    "\n",
    "list_domain = [col for col in df_test.columns if col.count('domain')]\n",
    "df_test[list_domain[0]].fillna('#', inplace=True)\n",
    "df_test[list_domain[0] +'_prefix'] = df_test[list_domain[0]].apply(lambda x: x.split('.')[0])\n",
    "df_test[list_domain[1]].fillna('#', inplace=True)\n",
    "df_test[list_domain[1] +'_prefix'] = df_test[list_domain[0]].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n",
    "          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n",
    "          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n",
    "          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n",
    "          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n",
    "          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n",
    "          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n",
    "          'anonymous.com': 'anonymous', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'anonymous', 'bellsouth.net': 'other',\n",
    "          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n",
    "          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n",
    "          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "\n",
    "\n",
    "list_domain = [col for col in df_train.columns if col.count('prefix') or col.count('_suffix') or col.count('_bin')]\n",
    "list_card = [col for col in df_train.columns if col.count('card')]\n",
    "list_addr = [col for col in df_train.columns if col.count('addr')]\n",
    "list_amt = ['cents', 'TransactionAmt']\n",
    "\n",
    "list_single = ['ProductCD']\\\n",
    "+ sorted(list_card)\\\n",
    "+ sorted(list_addr)\\\n",
    "+ sorted(list_domain)\\\n",
    "+ list_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combi = []\n",
    "for i in range(2, len(list_single)+1, 1):\n",
    "    list_combi.append(combinations(list_single, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Target Encoding\n",
    "# 当月のDataはDropして計算する(DT-MでGroupKするので)\n",
    "#========================================================================\n",
    "dir_save = 'create'\n",
    "cols_DTM = df_train['DT-M'].unique().tolist()\n",
    "\n",
    "# def parallel_TE(tmp_train, tmp_test, combi):\n",
    "    \n",
    "# for combi in list_combi:\n",
    "#     for col in tqdm(combi):\n",
    "for j in range(1):\n",
    "    for col in tqdm(list_single):\n",
    "\n",
    "        tmp_train = df_train\n",
    "        tmp_test = df_test\n",
    "\n",
    "        if str(type(col)).count('tuple'):\n",
    "            col = list(col)\n",
    "        if str(type(col)).count('list'):\n",
    "            pass\n",
    "        else:\n",
    "            col = [col]\n",
    "            \n",
    "        for c in col:\n",
    "            if str(tmp_train[c].dtype).count('int') or str(tmp_train[c].dtype).count('float'):\n",
    "                tmp_train[c].fillna(-999, inplace=True)\n",
    "                tmp_test[c].fillna(-999, inplace=True)\n",
    "            else:\n",
    "                tmp_train[c].fillna('#', inplace=True)\n",
    "                tmp_test[c].fillna('#', inplace=True)\n",
    "    \n",
    "        base_train = tmp_train[col + ['DT-M']]\n",
    "        base_test = tmp_test[col]\n",
    "        list_dtm = []\n",
    "    \n",
    "        fname = '-'.join(col)\n",
    "        feature_name = f'{fname}_fraud_mean'\n",
    "    \n",
    "        for dtm in cols_DTM + ['test']:\n",
    "    \n",
    "            # validationの期間を除く\n",
    "            if dtm != 'test':\n",
    "                df = tmp_train[tmp_train['DT-M']!=dtm].copy()\n",
    "            else:\n",
    "                df = tmp_train.copy()\n",
    "    \n",
    "            te_map = df.groupby(col)[COLUMN_TARGET].agg({\n",
    "                feature_name: 'mean'\n",
    "            })\n",
    "            cnt_map = df.groupby(col)[COLUMN_TARGET].agg({\n",
    "                'cnt': 'count'\n",
    "            })\n",
    "            # 母数が少ないのは平均埋め\n",
    "            df_te = pd.concat([te_map, cnt_map], axis=1)\n",
    "            df_te.loc[df_te[df_te['cnt']<100].index, feature_name] = tmp_train[COLUMN_TARGET].mean()\n",
    "            \n",
    "            if dtm != 'test':\n",
    "                te_map['DT-M'] = dtm\n",
    "                list_dtm.append(te_map)\n",
    "            else:\n",
    "                test_TE = te_map\n",
    "    \n",
    "        train_TE = pd.concat(list_dtm, axis=0)\n",
    "        result_train = base_train.merge(train_TE, how='left', on=col + ['DT-M'])\n",
    "        result_test = base_test.merge(test_TE, how='left', on=col)\n",
    "    \n",
    "        cols_save = [col for col in result_train.columns if col.count(f'_fraud_')]\n",
    "    \n",
    "        save_feature(result_train[cols_save], '524', dir_save, is_train=True, auto_type=True, list_ignore=COLUMNS_IGNORE)\n",
    "        save_feature(result_test[cols_save],  '524', dir_save, is_train=False, auto_type=True, list_ignore=COLUMNS_IGNORE)\n",
    "        \n",
    "# Parallel(4)([delayed(parallel_TE)(df_train, df_test, combi) for combi in list_combi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Target Encoding\n",
    "# 当月のDataはDropして計算する(DT-MでGroupKするので)\n",
    "#========================================================================\n",
    "dir_save = 'create'\n",
    "cols_DTM = df_train['DT-M'].unique().tolist()\n",
    "\n",
    "# def parallel_TE(tmp_train, tmp_test, combi):\n",
    "    \n",
    "for combi in list_combi:\n",
    "    for col in tqdm(combi):\n",
    "\n",
    "        tmp_train = df_train\n",
    "        tmp_test = df_test\n",
    "\n",
    "        if str(type(col)).count('tuple'):\n",
    "            col = list(col)\n",
    "        if str(type(col)).count('list'):\n",
    "            pass\n",
    "        else:\n",
    "            col = [col]\n",
    "            \n",
    "        for c in col:\n",
    "            if str(tmp_train[c].dtype).count('int') or str(tmp_train[c].dtype).count('float'):\n",
    "                tmp_train[c].fillna(-999, inplace=True)\n",
    "                tmp_test[c].fillna(-999, inplace=True)\n",
    "            else:\n",
    "                tmp_train[c].fillna('#', inplace=True)\n",
    "                tmp_test[c].fillna('#', inplace=True)\n",
    "    \n",
    "        base_train = tmp_train[col + ['DT-M']]\n",
    "        base_test = tmp_test[col]\n",
    "        list_dtm = []\n",
    "    \n",
    "        fname = '-'.join(col)\n",
    "        feature_name = f'{fname}_fraud_mean'\n",
    "    \n",
    "        for dtm in cols_DTM + ['test']:\n",
    "    \n",
    "            # validationの期間を除く\n",
    "            if dtm != 'test':\n",
    "                df = tmp_train[tmp_train['DT-M']!=dtm].copy()\n",
    "            else:\n",
    "                df = tmp_train.copy()\n",
    "    \n",
    "            te_map = df.groupby(col)[COLUMN_TARGET].agg({\n",
    "                feature_name: 'mean'\n",
    "            })\n",
    "            cnt_map = df.groupby(col)[COLUMN_TARGET].agg({\n",
    "                'cnt': 'count'\n",
    "            })\n",
    "            # 母数が少ないのは平均埋め\n",
    "            df_te = pd.concat([te_map, cnt_map], axis=1)\n",
    "            df_te.loc[df_te[df_te['cnt']<100].index, feature_name] = tmp_train[COLUMN_TARGET].mean()\n",
    "            \n",
    "            if dtm != 'test':\n",
    "                te_map['DT-M'] = dtm\n",
    "                list_dtm.append(te_map)\n",
    "            else:\n",
    "                test_TE = te_map\n",
    "    \n",
    "        train_TE = pd.concat(list_dtm, axis=0)\n",
    "        result_train = base_train.merge(train_TE, how='left', on=col + ['DT-M'])\n",
    "        result_test = base_test.merge(test_TE, how='left', on=col)\n",
    "    \n",
    "        cols_save = [col for col in result_train.columns if col.count(f'_fraud_')]\n",
    "    \n",
    "        save_feature(result_train[cols_save], f'524__combi{len(col)}', dir_save, is_train=True, auto_type=True, list_ignore=COLUMNS_IGNORE)\n",
    "        save_feature(result_test[cols_save],  f'524__combi{len(col)}', dir_save, is_train=False, auto_type=True, list_ignore=COLUMNS_IGNORE)\n",
    "        \n",
    "# Parallel(4)([delayed(parallel_TE)(df_train, df_test, combi) for combi in list_combi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
