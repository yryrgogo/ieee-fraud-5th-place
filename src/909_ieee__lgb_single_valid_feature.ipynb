{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 22:17:37,606 func.utils 347 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func.utils import timer, get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename, logger_func\n",
    "from ieee_train import eval_train, eval_check_feature\n",
    "from kaggle_utils import reduce_mem_usage, move_feature\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    logger\n",
    "except NameError:\n",
    "    logger = logger_func()\n",
    "    \n",
    "save_file_path = '../output/valid_single_feature.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMN_GROUP = 'DT-M'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, COLUMN_GROUP, 'is_train', 'date']\n",
    "\n",
    "paths_train = glob('../feature/raw_use/*_train.gz')\n",
    "paths_train += sorted(glob('../feature/org_use/*_train.gz'))\n",
    "\n",
    "df_train = parallel_load_data(paths_train)\n",
    "\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "df_train[COLUMN_GROUP] = group\n",
    "df_train = df_train[('2018-1' <= df_train[COLUMN_GROUP]) & (df_train[COLUMN_GROUP] <= '2018-5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  * Make Dataset] done in 2 s\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's auc: 0.913207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 22:17:59,291 func.utils 113 [INFO]    [<module>]   * 603__card2-addr1_V45_std_train Fold0:0.9132069728078794 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  * Train & Validation] done in 12 s\n",
      "[  * Make Dataset] done in 1 s\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's auc: 0.90305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 22:18:11,008 func.utils 113 [INFO]    [<module>]   * 603__card2-addr1_V45_std_train Fold1:0.9030497487881693 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  * Train & Validation] done in 11 s\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# Base Featureに検証用Feature Groupを追加して、スコアの変化を見る.\n",
    "# Baseより向上したFeature Groupのみ、追加検証を行う\n",
    "#========================================================================\n",
    "\n",
    "# 最初はbaseをTrueにして\n",
    "exp_no = 4\n",
    "is_base = [True, False][1]\n",
    "is_result = [True, False][0]\n",
    "to_dir = '../feature/check_trush/'\n",
    "\n",
    "def get_tree_importance(estimator, use_cols, importance_type=\"gain\"):\n",
    "    feim = estimator.feature_importance(importance_type=importance_type)\n",
    "    feim = pd.DataFrame([np.array(use_cols), feim]).T\n",
    "    feim.columns = ['feature', 'importance']\n",
    "    feim['importance'] = feim['importance'].astype('float32')\n",
    "    return feim\n",
    "\n",
    "\n",
    "valid_paths_train = sorted(glob('../feature/valid/*_train.gz'))\n",
    "loop_no = len(valid_paths_train)\n",
    "if len(valid_paths_train)==0 and (is_base or is_result):\n",
    "    loop_no = 1\n",
    "    \n",
    "for i in range(loop_no):\n",
    "    \n",
    "    if is_result:\n",
    "        valid_path = valid_paths_train\n",
    "    else:\n",
    "        valid_path = valid_paths_train[i:i+1]\n",
    "        \n",
    "    \n",
    "    if is_base or len(valid_path)==0:\n",
    "        tmp_train = df_train.copy()\n",
    "        feature_name = 'base'\n",
    "    else:\n",
    "        df_feat_train = parallel_load_data(valid_path)\n",
    "        tmp_train = df_train.join(df_feat_train)\n",
    "        feature_name = get_filename(valid_path[0])\n",
    "    \n",
    "    use_cols = [col for col in tmp_train.columns if col not in COLUMNS_IGNORE]\n",
    "        \n",
    "    for fold in range(2):\n",
    "\n",
    "        with timer('  * Make Dataset'):\n",
    "            if fold==0:\n",
    "                dataset = tmp_train[\n",
    "                    (tmp_train[COLUMN_GROUP] == '2018-2') | \n",
    "                    (tmp_train[COLUMN_GROUP] == '2018-3') | \n",
    "                    (tmp_train[COLUMN_GROUP] == '2018-5')]\n",
    "                train = dataset[('2018-2' <= dataset[COLUMN_GROUP]) & (dataset[COLUMN_GROUP] <= '2018-3')]\n",
    "                test  = dataset[dataset[COLUMN_GROUP] == '2018-5']\n",
    "            elif fold==1:\n",
    "                dataset = tmp_train[\n",
    "                    (df_train[COLUMN_GROUP] == '2018-1') | \n",
    "                    (df_train[COLUMN_GROUP] == '2018-2') | \n",
    "                    (df_train[COLUMN_GROUP] == '2018-4')]\n",
    "                train = dataset[('2018-1' <= dataset[COLUMN_GROUP]) & (dataset[COLUMN_GROUP] <= '2018-2')]\n",
    "                test  = dataset[dataset[COLUMN_GROUP] == '2018-4']\n",
    "        \n",
    "            Y_TRAIN = train[COLUMN_TARGET]\n",
    "            train.drop(COLUMN_TARGET, axis=1, inplace=True)\n",
    "        \n",
    "            Y_TEST = test[COLUMN_TARGET]\n",
    "            test.drop(COLUMN_TARGET, axis=1, inplace=True)\n",
    "        \n",
    "        start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())[:13]\n",
    "        params = {\n",
    "#             'n_jobs': 64,\n",
    "            'n_jobs': 20,\n",
    "            'seed': 1208,\n",
    "            'metric': 'auc',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves': 2**7-1,\n",
    "            'max_depth': -1,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'colsample_bytree' : 1.0,\n",
    "            'lambda_l1' : 0.1,\n",
    "            'lambda_l2' : 1.0,\n",
    "            'learning_rate' : 0.1,\n",
    "        }\n",
    "        \n",
    "        x_train = train[use_cols]\n",
    "        y_train = Y_TRAIN\n",
    "        x_valid = test[use_cols]\n",
    "        y_valid = Y_TEST\n",
    "        early_stopping_rounds=20\n",
    "        num_boost_round=3500\n",
    "        metric = 'auc'\n",
    "        params['metric'] = metric\n",
    "        \n",
    "        #========================================================================\n",
    "        # Fitting\n",
    "        #========================================================================\n",
    "        lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "        lgb_valid = lgb.Dataset(data=x_valid, label=y_valid)\n",
    "        \n",
    "        with timer(\"  * Train & Validation\"):\n",
    "            estimator = lgb.train(\n",
    "                params = params,\n",
    "                train_set = lgb_train,\n",
    "                valid_sets = lgb_valid,\n",
    "                early_stopping_rounds = early_stopping_rounds,\n",
    "                num_boost_round = num_boost_round,\n",
    "                verbose_eval = 200\n",
    "            )\n",
    "            best_iter = estimator.best_iteration\n",
    "        \n",
    "            oof_pred = estimator.predict(x_valid)\n",
    "            score = roc_auc_score(y_valid, oof_pred)\n",
    "            cvs = str(score).replace('.', '-')\n",
    "            logger.info(f\"  * {feature_name} Fold{fold}:{score}\")\n",
    "            \n",
    "            if not is_result:\n",
    "                with open(save_file_path, 'a') as f:\n",
    "                    line = f'{exp_no},{fold},{feature_name},{score}\\n'\n",
    "                    f.write(line)\n",
    "            \n",
    "#             feim = get_tree_importance(estimator=estimator, use_cols=x_train.columns)\n",
    "#             feim.sort_values(by='importance', ascending=False, inplace=True)\n",
    "#             feim['is_valid'] = feim['feature'].map(valid_map)\n",
    "\n",
    "    if is_base or is_result:\n",
    "        sys.exit()\n",
    "        \n",
    "    #========================================================================\n",
    "    # PostProcess\n",
    "    #========================================================================\n",
    "    with timer(\"  * PostProcess\"):\n",
    "        for path in valid_path:\n",
    "            try:\n",
    "                shutil.move(path, to_dir)\n",
    "            except FileNotFoundError:\n",
    "                print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_name = result.index[6].replace('_train', '')\n",
    "feature_name = '603__card2-addr1_V45_std'\n",
    "from_dir = 'check_trush'\n",
    "to_dir = 'valid'\n",
    "move_feature([feature_name], from_dir, to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score_0</th>\n",
       "      <th>score_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>605__card4-P_emaildomain_prefix_V90_std_train</th>\n",
       "      <td>0.914543</td>\n",
       "      <td>0.904727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603__card2-addr1_V45_std_train</th>\n",
       "      <td>0.914239</td>\n",
       "      <td>0.904682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603__card4-addr1_C3_sum_train</th>\n",
       "      <td>0.914077</td>\n",
       "      <td>0.905081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.914077</td>\n",
       "      <td>0.903281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                score_0   score_1\n",
       "feature                                                          \n",
       "605__card4-P_emaildomain_prefix_V90_std_train  0.914543  0.904727\n",
       "603__card2-addr1_V45_std_train                 0.914239  0.904682\n",
       "603__card4-addr1_C3_sum_train                  0.914077  0.905081\n",
       "base                                           0.914077  0.903281"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_no = 4\n",
    "df_feature = pd.read_csv(save_file_path)\n",
    "df_feature = df_feature[df_feature['exp_no']==exp_no]\n",
    "df_feature.drop('exp_no', axis=1, inplace=True)\n",
    "fold0 = df_feature[df_feature.fold==0].set_index('feature').drop('fold', axis=1)\n",
    "fold0.columns = ['score_0']\n",
    "fold1 = df_feature[df_feature.fold==1].set_index('feature').drop('fold', axis=1)\n",
    "fold1.columns = ['score_1']\n",
    "\n",
    "result = fold0.join(fold1)\n",
    "result = result.drop_duplicates()\n",
    "result.sort_values(by='score_0', ascending=False, inplace=True)\n",
    "base_0 = result.loc['base']['score_0']\n",
    "base_1 = result.loc['base']['score_1']\n",
    "result[(result['score_0']>=base_0) & (result['score_1']>=base_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use_less_feature in result[(result['score_0']<base_0) & (result['score_1']<base_1)].index:\n",
    "    move_feature([use_less_feature.replace('_train', '')], 'check_trush', 'valid_trush')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
