{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func.utils import get_numeric_features, get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename\n",
    "from func.time_utils import date_add_days, date_add_times\n",
    "from func.ml_utils import save_feature, get_cnt_feature, get_dummie_feature, get_label_feature, get_factorize_feature\n",
    "from func.parallel_utils import get_parallel_arg_list\n",
    "from joblib import delayed, Parallel\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-85:\n",
      "Process ForkPoolWorker-92:\n",
      "Process ForkPoolWorker-170:\n",
      "Process ForkPoolWorker-191:\n",
      "Process ForkPoolWorker-185:\n",
      "Process ForkPoolWorker-186:\n",
      "Process ForkPoolWorker-156:\n",
      "Process ForkPoolWorker-192:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-184:\n",
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-158:\n",
      "Process ForkPoolWorker-59:\n",
      "Process ForkPoolWorker-65:\n",
      "Process ForkPoolWorker-96:\n",
      "Process ForkPoolWorker-79:\n",
      "Process ForkPoolWorker-94:\n",
      "Process ForkPoolWorker-168:\n",
      "Process ForkPoolWorker-148:\n",
      "Process ForkPoolWorker-66:\n",
      "Process ForkPoolWorker-137:\n",
      "Process ForkPoolWorker-139:\n",
      "Process ForkPoolWorker-167:\n",
      "Process ForkPoolWorker-130:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-141:\n",
      "Process ForkPoolWorker-67:\n",
      "Process ForkPoolWorker-174:\n",
      "Process ForkPoolWorker-89:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-72:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-36:\n",
      "Process ForkPoolWorker-58:\n",
      "Process ForkPoolWorker-63:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-42:\n"
     ]
    }
   ],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, 'ProductCD']\n",
    "\n",
    "train_paths = glob('../feature/eda_base/*_train.gz')\n",
    "test_paths  = glob('../feature/eda_base/*_test.gz')\n",
    "\n",
    "train_paths = [path for path in train_paths \n",
    "               if (path.count(COLUMN_DT) \n",
    "               or path.count(COLUMN_ID)\n",
    "               or path.count(COLUMN_TARGET)\n",
    "               or path.count('ProductCD_t')\n",
    "               or path.count('uid')\n",
    "               or path.count('C')\n",
    "               or path.count('V')\n",
    "               )\n",
    "               and not path.count('bin_')\n",
    "               and not path.count('fill_')\n",
    "               and not path.count('129')\n",
    "              ]\n",
    "test_paths = [path for path in test_paths \n",
    "               if (path.count(COLUMN_DT) \n",
    "               or path.count(COLUMN_ID)\n",
    "               or path.count(COLUMN_TARGET)\n",
    "               or path.count('ProductCD_t')\n",
    "               or path.count('uid')\n",
    "               or path.count('C')\n",
    "               or path.count('V')\n",
    "               )\n",
    "               and not path.count('bin_')\n",
    "               and not path.count('fill_')\n",
    "               and not path.count('129')\n",
    "              ]\n",
    "\n",
    "df_train = parallel_load_data(train_paths)\n",
    "df_test = parallel_load_data(test_paths)\n",
    "train_length = df_train.shape[0]\n",
    "cols_uid = [col for col in df_train.columns if col.count('uid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_uid_agg(tmp_train, tmp_test, uid, cols_feature, list_agg):\n",
    "    for feature in tqdm(cols_feature):\n",
    "        for agg_type in list_agg:\n",
    "            new_col_name = uid.replace('130__', '') + '_' + feature + '_' + agg_type\n",
    "            temp_df = pd.concat([tmp_train[[uid, feature]], tmp_test[[uid, feature]]])\n",
    "            temp_df = temp_df.groupby([uid])[feature].agg([agg_type]).reset_index().rename(\n",
    "                                                    columns={agg_type: new_col_name})\n",
    "\n",
    "            temp_df.index = list(temp_df[uid])\n",
    "            temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "            tmp_train[new_col_name] = tmp_train[uid].map(temp_df)\n",
    "            tmp_test[new_col_name]  = tmp_test[uid].map(temp_df)\n",
    "            \n",
    "            save_feature(tmp_train[[new_col_name]], prefix, dir_save, is_train=True, auto_type=False, list_ignore=COLUMNS_IGNORE)\n",
    "            save_feature(tmp_test[[new_col_name]],  prefix, dir_save, is_train=False, auto_type=False, list_ignore=COLUMNS_IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_C = [col for col in df_train.columns if col.startswith('C')]\n",
    "cols_V = [col for col in df_train.columns if col.startswith('V')]\n",
    "# cols_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uid_feature(cols_feature, list_agg):\n",
    "    Parallel(len(cols_uid))([\n",
    "        delayed(parallel_uid_agg)(\n",
    "            df_train[[uid] + cols_feature], df_test[[uid] + cols_feature], uid, cols_feature, list_agg\n",
    "        ) for uid in cols_uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '612'\n",
    "dir_save = 'create'\n",
    "list_agg = ['mean', 'std']\n",
    "\n",
    "# C\n",
    "# make_uid_feature(cols_C, list_agg)\n",
    "# V\n",
    "make_uid_feature(cols_V, list_agg)\n",
    "# D\n",
    "# make_uid_feature(cols_D, list_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
