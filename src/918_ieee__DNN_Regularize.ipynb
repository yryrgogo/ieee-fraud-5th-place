{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-23 14:30:41,106 func.utils 347 [INFO]    [logger_func] start \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from func.utils import get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename, logger_func, timer\n",
    "from ieee_train import eval_train, eval_check_feature\n",
    "from kaggle_utils import reduce_mem_usage, move_feature\n",
    "logger = logger_func()\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.models import *\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_fixlen_feature_names\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([np.inf, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(a==np.inf, 2, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMN_GROUP = 'DT-M'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET, COLUMN_GROUP, 'is_train', 'date']\n",
    "\n",
    "def filter_feature(path):\n",
    "    if path.count(''):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "paths_train = glob('../feature/raw_use/*_train.gz')\n",
    "paths_test  = glob('../feature/raw_use/*_test.gz')\n",
    "# paths_train = glob('../submit/re_sub/*_train.gz')\n",
    "# paths_test  = glob('../submit/re_sub/*_test.gz')\n",
    "# paths_train += glob('../submit/add_feature/*_train.gz')\n",
    "# paths_test  += glob('../submit/add_feature/*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/531*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/531*_test.gz')\n",
    "# paths_train += glob('../feature/valid_use/532*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_use/532*_test.gz')\n",
    "# paths_train += glob('../feature/valid_trush/532*uid3*_train.gz')\n",
    "# paths_test  += glob('../feature/valid_trush/532*uid3*_test.gz')\n",
    "\n",
    "paths_train_feature = []\n",
    "paths_test_feature  = []\n",
    "\n",
    "df_train = parallel_load_data(paths_train)\n",
    "df_test  = parallel_load_data(paths_test)\n",
    "Y = df_train[COLUMN_TARGET]\n",
    "df_train.drop(COLUMN_TARGET, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "sparse_features = []\n",
    "dense_features = data.columns\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "\n",
    "with timer(\"Fill Na\"):\n",
    "    for col in tqdm(dense_features):\n",
    "        avg = data[col].mean()\n",
    "        data[dense_features] = data[dense_features].fillna(avg, )\n",
    "    \n",
    "target = [COLUMN_TARGET]\n",
    "\n",
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "    \n",
    "    \n",
    "with timer(\"Min Max Scalar\"):\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    dense_features = [col for col in dense_features if not col.count('513__D2-D4__ratio__ProductCD-W')]\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                          for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                                                          for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns    = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "fixlen_feature_names = get_fixlen_feature_names(\n",
    "    linear_feature_columns + dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "df_train = data.iloc[:len(df_train)]\n",
    "df_test  = data.iloc[len(df_train):]\n",
    "\n",
    "n_splits = 6\n",
    "group_kfold_path = '../input/0908_ieee__DT-M_GroupKFold.gz'\n",
    "group = read_pkl_gzip(group_kfold_path)\n",
    "df_train[COLUMN_GROUP] = group\n",
    "\n",
    "kfold = list(GroupKFold(n_splits=n_splits).split(df_train, Y, df_train[COLUMN_GROUP]))\n",
    "print(\"Set Kfold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.generate input data for model\n",
    "\n",
    "use_cols = dense_features\n",
    "oof_pred = np.zeros(df_train.shape[0])\n",
    "x_test = df_test[use_cols]\n",
    "test_preds = []\n",
    "test_model_input = [x_test[name] for name in fixlen_feature_names]\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(kfold):\n",
    "    \n",
    "    with timer(\"Preset Dataset.\"):\n",
    "        x_train = df_train.iloc[trn_idx][use_cols]\n",
    "        y_train = Y.iloc[trn_idx]\n",
    "        x_valid = df_train.iloc[val_idx][use_cols]\n",
    "        y_valid = Y.iloc[val_idx]\n",
    "\n",
    "        train_model_input = [x_train[name] for name in fixlen_feature_names]\n",
    "        valid_model_input = [x_valid[name] for name in fixlen_feature_names]\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    \n",
    "    print(\"Start Train and Predict.\")\n",
    "    with timer(\"Fitting\"):\n",
    "\n",
    "        device = 'cpu'\n",
    "        use_cuda = True\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            print('cuda ready...')\n",
    "            device = 'cuda:0'\n",
    "\n",
    "        model = DeepFM(\n",
    "            linear_feature_columns=linear_feature_columns,\n",
    "            dnn_feature_columns=dnn_feature_columns,\n",
    "            task='binary',\n",
    "            l2_reg_embedding=1e-5,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            \"adagrad\",\n",
    "            \"binary_crossentropy\",\n",
    "            metrics=[\"binary_crossentropy\", \"auc\"],\n",
    "        )\n",
    "        model.fit(\n",
    "            train_model_input,\n",
    "            y_train.values,\n",
    "            batch_size=1024,\n",
    "            epochs=7,\n",
    "            validation_split=0.0,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "    with timer(\"Predict\"):\n",
    "        \n",
    "        pred_ans = model.predict(valid_model_input, 256)\n",
    "        print(\"\")\n",
    "        print(\"test LogLoss\", round(log_loss(y_valid, pred_ans), 4))\n",
    "        print(\"test AUC\", round(roc_auc_score(y_valid.values, pred_ans), 4))\n",
    "        \n",
    "        oof_pred[val_idx] = pred_ans\n",
    "        \n",
    "        test_pred = model.predict(test_model_input, 256)\n",
    "        test_preds.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01531531],\n",
       "       [0.01715467],\n",
       "       [0.00604828],\n",
       "       ...,\n",
       "       [0.02184101],\n",
       "       [0.00815489],\n",
       "       [0.01425854]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
