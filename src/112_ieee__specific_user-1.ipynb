{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:fbprophet:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from func.utils import get_categorical_features, read_pkl_gzip, to_pkl_gzip, parallel_load_data, get_filename\n",
    "from func.time_utils import date_add_days, diff_of_days, diff_of_times\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from train_prophet import main_prophet\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_ID = 'TransactionID'\n",
    "COLUMN_DT = 'TransactionDT'\n",
    "COLUMN_TARGET = 'isFraud'\n",
    "COLUMNS_IGNORE = [COLUMN_ID, COLUMN_DT, COLUMN_TARGET]\n",
    "\n",
    "train_paths = glob('../feature/raw_main/*_train.gz')\n",
    "test_paths = glob('../feature/raw_main/*_test.gz')\n",
    "\n",
    "train_paths = [path for path in train_paths \n",
    "               if path.count('DT') \n",
    "               or path.count('day_no') \n",
    "               or path.count('Amt')\n",
    "               or path.count('Fraud') \n",
    "               or path.count('D')\n",
    "               or path.count('card')\n",
    "               or path.count('addr')\n",
    "               or path.count('Reg')\n",
    "               or path.count('P_email')\n",
    "               or path.count('R_email')\n",
    "               or path.count('M')\n",
    "               or path.count('Product')\n",
    "              ]\n",
    "test_paths  = [path for path in test_paths  \n",
    "               if path.count('DT') \n",
    "               or path.count('day_no') \n",
    "               or path.count('Amt')\n",
    "               or path.count('Fraud') \n",
    "               or path.count('D')\n",
    "               or path.count('card')\n",
    "               or path.count('addr')\n",
    "               or path.count('Reg')\n",
    "               or path.count('P_email')\n",
    "               or path.count('R_email')\n",
    "               or path.count('M')\n",
    "               or path.count('Product')\n",
    "              ]\n",
    "\n",
    "train_df = parallel_load_data(train_paths)\n",
    "test_df = parallel_load_data(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2017-11-04'\n",
    "# START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "train_df['datetime'] = train_df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x) - datetime.timedelta(seconds = 14400) ))\n",
    "test_df['datetime'] = test_df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x) - datetime.timedelta(seconds = 14400) ))\n",
    "train_df['datetime'].fillna(datetime.date(2020, 1, 1), inplace=True)\n",
    "test_df['datetime'].fillna(datetime.date(2020, 1, 1), inplace=True)\n",
    "train_df['date'] = train_df['datetime'].map(lambda x: x.date())\n",
    "test_df['date']  =  test_df['datetime'].map(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 590540/590540 [00:02<00:00, 245834.16it/s]\n",
      "100%|██████████| 506691/506691 [00:02<00:00, 248603.76it/s]\n"
     ]
    }
   ],
   "source": [
    "list_regist = []\n",
    "for d, diff in tqdm(train_df[['date', 'D1']].values):\n",
    "    if diff < 999999:\n",
    "        regist = date_add_days(d, -1*diff)\n",
    "    else:\n",
    "        regist = date_add_days(d, 0)\n",
    "    list_regist.append(str(regist))\n",
    "\n",
    "train_df['Regist_date'] = list_regist\n",
    "    \n",
    "list_regist = []\n",
    "for d, diff in tqdm(test_df[['date', 'D1']].values):\n",
    "    if diff < 999999:\n",
    "        regist = date_add_days(d, -1*diff)\n",
    "    else:\n",
    "        regist = date_add_days(d, 0)\n",
    "    list_regist.append(str(regist))\n",
    "\n",
    "test_df['Regist_date'] = list_regist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "異なるuser keyで様々なユーザー特定パターン（正確でなくてもよい）を作り、それぞれのパターンで特徴を作る  \n",
    "正確でなくてもよい理由としては、粒度を粗くしてもある条件を外した特徴量になるだけなので、問題ない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = [\n",
    "    'ProductCD',\n",
    "    'Regist_date'\n",
    "]\n",
    "list_card = [\n",
    "    'card1',\n",
    "    'card2',\n",
    "    'card3',\n",
    "    'card4',\n",
    "    'card5',\n",
    "    'card6',\n",
    "]\n",
    "list_addr = [\n",
    "    'addr1',\n",
    "    'addr2',\n",
    "]\n",
    "list_pemail = [\n",
    "    'P_emaildomain'\n",
    "]\n",
    "list_M = [col for col in data.columns if col.startswith('M')]\n",
    "\n",
    "user_keys__card = user_info + list_card\n",
    "\n",
    "user_keys__card_addr = user_info + list_card + list_addr\n",
    "user_keys__card_pemail = user_info + list_card + list_pemail\n",
    "user_keys__card_M = user_info + list_card + list_M\n",
    "\n",
    "user_keys__card_addr_pemail = user_info + list_card + list_addr + list_pemail\n",
    "user_keys__card_addr_M = user_info + list_card + list_addr + list_M\n",
    "user_keys__card_pemail_M = user_info + list_card + list_pemail + list_M\n",
    "\n",
    "user_keys__card_addr_pemail_M = user_info + list_card + list_addr + list_pemail + list_M\n",
    "\n",
    "list_user_keys = [\n",
    "  ['user_keys__card',      user_keys__card,],\n",
    "  ['user_keys__card_addr',      user_keys__card_addr,],\n",
    "  ['user_keys__card_pemail',      user_keys__card_pemail,],\n",
    "  ['user_keys__card_M',      user_keys__card_M,],\n",
    "  ['user_keys__card_addr_M',      user_keys__card_addr_M,],\n",
    "  ['user_keys__card_addr_pemail',      user_keys__card_addr_pemail,],\n",
    "  ['user_keys__card_pemail_M',      user_keys__card_pemail_M,],\n",
    "  ['user_keys__card_addr_pemail_M',      user_keys__card_addr_pemail_M,],\n",
    "]\n",
    "check_Ds = ['D1', 'D2', 'D3', 'D4', 'D5', 'datetime']\n",
    "\n",
    "data['D1'].fillna(-1, inplace=True)\n",
    "data[user_info + list_card] = data[user_info + list_card].astype('str').fillna('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_user_id(data):\n",
    "    \n",
    "    uid = 'user_id'\n",
    "    df = data.copy()\n",
    "\n",
    "    for col in user_keys:\n",
    "        df[col].fillna('#', inplace=True)\n",
    "        \n",
    "    df[user_keys] =  df[user_keys].astype('str')\n",
    "    \n",
    "    user_df = df.groupby(user_keys)[COLUMN_ID].min()\n",
    "    user_df = user_df.to_frame(uid)\n",
    "    user_df = df.set_index(user_keys).join(user_df)\n",
    "    \n",
    "    check_cols = [COLUMN_ID, uid, COLUMN_TARGET, 'Regist_date'] + check_Ds\n",
    "    user_df = user_df.reset_index()[check_cols]\n",
    "    user_df.sort_values(by=[uid, 'datetime'], inplace=True)\n",
    "    \n",
    "    col_same_user = f'same_{uid}'\n",
    "    \n",
    "    \n",
    "    if uid not in user_df.columns:\n",
    "        user_df.reset_index(inplace=True)\n",
    "    \n",
    "    col_same_user = f'same_{uid}'\n",
    "    max_fraud = user_df.groupby(uid)[COLUMN_TARGET].max()\n",
    "    if uid in user_df.columns:\n",
    "        user_df.set_index(uid, inplace=True)\n",
    "    user_df['max_Fraud'] = max_fraud\n",
    "    \n",
    "    # Fraudを持つ可能性のあるユーザーのみで考える\n",
    "    user_df.reset_index(inplace=True)\n",
    "    user_df['some_fraud_user'] = (user_df[COLUMN_TARGET] != user_df['max_Fraud'])*1\n",
    "#     diff_fraud_users = user_df[user_df[COLUMN_TARGET] != user_df['max_Fraud']].reset_index()[uid].unique()\n",
    "#     user_df = user_df[user_df[uid].isin(diff_fraud_users)]\n",
    "    user_df.sort_values(by=[uid, 'datetime'], inplace=True)\n",
    "    \n",
    "    user_df['date'] = user_df['datetime'].map(lambda x: x.date())\n",
    "    user_df['diff_from_regist'] = user_df['datetime'] - user_df['Regist_date'].map(lambda x: datetime.datetime(int(x[:4]), int(x[5:7]), int(x[8:10]) ) )\n",
    "    user_df['diff_from_regist'] = user_df['diff_from_regist'].map(lambda x: x.days)\n",
    "    \n",
    "    user_df['Regist_date_add_D1'] = user_df[['Regist_date', 'D1']].apply(lambda x: date_add_days(x[0], x[1]), axis=1)\n",
    "    user_df['Regist_date_add_D1-D3'] = user_df[['Regist_date_add_D1', 'D3']].apply(lambda x: date_add_days(x[0], -1*x[1]), axis=1)\n",
    "    user_df['Regist_date_add_D1-D3-1'] = user_df[['Regist_date_add_D1', 'D3']].apply(lambda x: date_add_days(x[0], -1*x[1]-1), axis=1)\n",
    "    \n",
    "    user_df['date__before1'] = user_df.groupby(uid)['date'].shift(1)\n",
    "    user_df['date__before2'] = user_df.groupby(uid)['date'].shift(2)\n",
    "    user_df['date__before3'] = user_df.groupby(uid)['date'].shift(3)\n",
    "    user_df['D1__before1']   = user_df.groupby(uid)['D1'].shift(1)\n",
    "    user_df['D1__before2']   = user_df.groupby(uid)['D1'].shift(2)\n",
    "    \n",
    "    user_df['diff_D1__before1'] = user_df[['D1', 'D1__before1']].apply(lambda x: x[0] - x[1] , axis=1)\n",
    "    user_df['diff_D1__before2'] = user_df[['D1', 'D1__before2']].apply(lambda x: x[0] - x[1] , axis=1)\n",
    "    user_df['diff_day__before1'] = user_df[['date', 'date__before1']].apply(lambda x: diff_of_days(x[0], x[1]) , axis=1)\n",
    "    user_df['diff_day__before2'] = user_df[['date', 'date__before2']].apply(lambda x: diff_of_days(x[0], x[1]) , axis=1)\n",
    "    user_df['diff_day__before3'] = user_df[['date', 'date__before3']].apply(lambda x: diff_of_days(x[0], x[1]) , axis=1)\n",
    "    \n",
    "    # ユーザーIDの作成\n",
    "    cnt = 0\n",
    "    p_cnt = 0\n",
    "    list_user = []\n",
    "    for user, d3, b1, b2, b3, D1_b1, D1_b2 in user_df[[uid, 'D3', 'diff_day__before1', 'diff_day__before2', 'diff_day__before3', 'diff_D1__before1', 'diff_D1__before2']].values:\n",
    "        p_cnt+=1\n",
    "#         if d3==b3 or d3==b3-1:\n",
    "        if d3==b3:\n",
    "            current_user = b3_user\n",
    "#         elif d3==b2 or d3==b2-1:\n",
    "        elif d3==b2:\n",
    "            current_user = b2_user\n",
    "#         elif d3==b1 or d3==b1-1:\n",
    "        elif d3==b1:\n",
    "            current_user = b1_user\n",
    "#         elif D1_b1 == b1 and D1_b1 != 0:\n",
    "#             current_user = b1_user\n",
    "#         elif D1_b2 == b2 and D1_b2 != 0:\n",
    "#             current_user = b2_user\n",
    "        else:\n",
    "            cnt += 1\n",
    "        current_user = cnt\n",
    "            \n",
    "        if p_cnt>=3:\n",
    "            b3_user = b2_user\n",
    "        if p_cnt>=2:\n",
    "            b2_user = b1_user\n",
    "        b1_user = cnt\n",
    "        list_user.append(current_user)\n",
    "        \n",
    "    user_df[col_same_user] = list_user\n",
    "    print(f\"Extract Same User ID.\", user_df.shape)\n",
    "    user_df = user_df[[COLUMN_ID, col_same_user, uid]]\n",
    "    \n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█▎        | 1/8 [08:55<1:02:30, 535.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [18:00<53:51, 538.58s/it]  \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 38%|███▊      | 3/8 [41:16<1:06:18, 795.64s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 4/8 [50:30<48:12, 723.21s/it]  \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 62%|██████▎   | 5/8 [59:50<33:42, 674.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [1:08:55<21:11, 635.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 88%|████████▊ | 7/8 [1:18:06<10:10, 610.11s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Same User ID. (1097231, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 8/8 [1:29:58<00:00, 640.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for pattern, user_keys in tqdm(list_user_keys):\n",
    "    df_new_user = create_new_user_id(data)\n",
    "    df_new_user.to_csv(f'../output/0830_ieee__same_user__pattern-{pattern}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
